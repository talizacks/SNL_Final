{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b641cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # library for data analysis\n",
    "import requests # library to handle requests\n",
    "from bs4 import BeautifulSoup # library to parse HTML documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "93e7a85c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "# get the response in the form of html\n",
    "wikiurl=\"https://en.wikipedia.org/wiki/Fred_Armisen_filmography\"\n",
    "table_class=\"wikitable sortable jquery-tablesorter\"\n",
    "response=requests.get(wikiurl)\n",
    "print(response.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "06063da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse data from the html into a beautifulsoup object\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "indiatable=soup.find('table',{'class':\"wikitable sortable\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9c594e75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Reverb',\n",
       " 'Fred',\n",
       " 'Late Friday',\n",
       " 'Premium Blend',\n",
       " 'Next!',\n",
       " 'Late World with Zach',\n",
       " 'Saturday Night Live',\n",
       " 'Crank Yankers',\n",
       " 'Comedy Lab',\n",
       " 'Aqua Teen Hunger Force',\n",
       " 'New York Noise',\n",
       " 'Squidbillies',\n",
       " 'Freak Show',\n",
       " 'Night of Too Many Stars',\n",
       " 'Tom Goes to the Mayor',\n",
       " '30 Rock',\n",
       " 'Tim and Eric Nite Live!',\n",
       " 'Human Giant',\n",
       " 'Yo Gabba Gabba!',\n",
       " 'The Sarah Silverman Program',\n",
       " 'Blue Man Group: How to Be a Megastar 2.0',\n",
       " 'Saturday Night Live Weekend Update Thursday',\n",
       " 'Tim and Eric Awesome Show, Great Job!',\n",
       " 'Parks and Recreation',\n",
       " 'Ugly Americans',\n",
       " 'Portlandia',\n",
       " 'The Looney Tunes Show',\n",
       " 'The Soup',\n",
       " 'Up All Night',\n",
       " 'Late Night with Jimmy Fallon',\n",
       " 'The Simpsons',\n",
       " 'Unsupervised',\n",
       " \"Bob's Burgers\",\n",
       " 'Conan',\n",
       " 'Kroll Show',\n",
       " 'Jimmy Kimmel Live!',\n",
       " 'Out There',\n",
       " 'The Awesomes',\n",
       " 'Brooklyn Nine-Nine',\n",
       " 'Super Fun Night',\n",
       " 'Broad City',\n",
       " 'Late Night with Seth Meyers',\n",
       " 'House of Lies',\n",
       " 'Chozen',\n",
       " 'Modern Family',\n",
       " 'Comedy Bang! Bang!',\n",
       " 'Comedians in Cars Getting Coffee',\n",
       " 'Archer',\n",
       " 'Mike Tyson Mysteries',\n",
       " \"Elf: Buddy's Musical Christmas\",\n",
       " '30th Independent Spirit Awards',\n",
       " 'Man Seeking Woman',\n",
       " '7 Days in Hell',\n",
       " 'The Jim Gaffigan Show',\n",
       " 'Difficult People',\n",
       " 'Documentary Now!',\n",
       " 'Robot Chicken',\n",
       " 'New Girl',\n",
       " 'Unbreakable Kimmy Schmidt',\n",
       " 'Man Seeking Woman',\n",
       " 'Blunt Talk',\n",
       " \"Michael Bolton's Big, Sexy Valentine's Day Special\",\n",
       " 'Nature Cat',\n",
       " 'Son of Zorn',\n",
       " 'Animals.',\n",
       " 'Comrade Detective',\n",
       " 'Big Mouth',\n",
       " 'Lady Dynamite',\n",
       " 'The Last Man on Earth',\n",
       " 'I Love You, America with Sarah Silverman',\n",
       " 'A Christmas Story Live!',\n",
       " 'Final Space',\n",
       " 'Splitting Up Together',\n",
       " 'Forever',\n",
       " 'Fortune Rookie',\n",
       " 'Crazy Ex-Girlfriend',\n",
       " 'At Home with Amy Sedaris',\n",
       " 'Los Espookys',\n",
       " 'Superstore',\n",
       " 'The Kacey Musgraves Christmas Show',\n",
       " 'Curb Your Enthusiasm',\n",
       " 'Miracle Workers',\n",
       " 'Central Park',\n",
       " 'Elena of Avalor',\n",
       " 'Mapleworth Murders[2]',\n",
       " 'Moonbase 8',\n",
       " \"Sarah Cooper: Everything's Fine\",\n",
       " 'Shrill',\n",
       " 'Bless the Harts',\n",
       " 'Schmigadoon!',\n",
       " 'Centaurworld',\n",
       " 'Toast of Tinseltown',\n",
       " 'The Kids in the Hall']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_html(str(indiatable))\n",
    "# convert list to dataframe\n",
    "df=pd.DataFrame(df[0])\n",
    "tv = []\n",
    "for title in df['Title']:\n",
    "    tv.append(title)\n",
    "tv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b0ac4b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "indiatable=soup.find('table',{'class':\"wikitable\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "af79726c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Guide to Music and South by Southwest',\n",
       " \"Fred Armisen's Guide to Dance and Self-Defense[1]\",\n",
       " 'I Am Trying to Break Your Heart: A Film About Wilco',\n",
       " 'Like Mike',\n",
       " 'Frank International Film Festival',\n",
       " 'Melvin Goes to Dinner',\n",
       " 'Anchorman: The Legend of Ron Burgundy',\n",
       " 'Eurotrip',\n",
       " 'Wake Up, Ron Burgundy: The Lost Movie',\n",
       " 'Deuce Bigalow: European Gigolo',\n",
       " 'Deck the Halls',\n",
       " 'The Ex',\n",
       " 'Griffin & Phoenix',\n",
       " 'Kiss Me Again',\n",
       " 'Tenacious D in The Pick of Destiny',\n",
       " 'Aqua Teen Hunger Force Colon Movie Film for Theaters',\n",
       " 'Baby Mama',\n",
       " 'Christmas on Mars',\n",
       " 'The Promotion',\n",
       " 'The Rocker',\n",
       " 'Bang Blow & Stroke',\n",
       " 'Confessions of a Shopaholic',\n",
       " 'Post Grad',\n",
       " 'Cop Out',\n",
       " 'Our Family Wedding',\n",
       " 'Easy A',\n",
       " 'Cats & Dogs: The Revenge of Kitty Galore',\n",
       " 'Presidential Reunion',\n",
       " 'The Smurfs',\n",
       " 'The Smurfs: A Christmas Carol',\n",
       " 'The Dictator',\n",
       " 'Fun World',\n",
       " 'The Smurfs 2',\n",
       " 'The Smurfs: The Legend of Smurfy Hollow',\n",
       " 'Salad Days',\n",
       " 'Addicted to Fresno',\n",
       " 'Staten Island Summer',\n",
       " 'Looney Tunes: Rabbits Run',\n",
       " \"The Damned: Don't You Wish That We Were Dead\",\n",
       " 'Zoolander 2',\n",
       " 'Ordinary World',\n",
       " 'The Little Hours',\n",
       " 'Take the 10',\n",
       " 'Band Aid',\n",
       " 'The House of Tomorrow',\n",
       " 'Battle of the Sexes',\n",
       " 'The Lego Ninjago Movie',\n",
       " \"Zane's Stand Up Promo\",\n",
       " 'Game Over, Man!',\n",
       " 'Jay and Silent Bob Reboot',\n",
       " 'All Together Now',\n",
       " 'How It Ends',\n",
       " 'Too Late',\n",
       " 'The Mitchells vs. the Machines',\n",
       " 'Spin Me Round',\n",
       " 'The Bubble',\n",
       " 'Untitled Mario film']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_html(str(indiatable))\n",
    "# convert list to dataframe\n",
    "df=pd.DataFrame(df[0])\n",
    "Fred_Armisten_film = []\n",
    "for title in df['Title']:\n",
    "    Fred_Armisten_films.append(title)\n",
    "Fred_Armisten_film"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "fbc58e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "Fred_Armisten_film = [\"Guide to Music and South by Southwest\",\"Fred Armisen's Guide to Dance and Self-Defense\",\"I Am Trying to Break Your Heart: A Film About Wilco\",\"Like Mike\",\"Frank International Film Festival\",\"Melvin Goes to Dinner\",\"Anchorman: The Legend of Ron Burgundy\",\"Eurotrip\",\"Wake Up, Ron Burgundy: The Lost Movie\",\"Deuce Bigalow: European Gigolo\",\"Deck the Halls\",\"The Ex\",\"Griffin & Phoenix\",\"Kiss Me Again\",\"Tenacious D in The Pick of Destiny\",\"Aqua Teen Hunger Force Colon Movie Film for Theaters\",\"Baby Mama\",\"Christmas on Mars\",\"The Promotion\",\"The Rocker\",\"Bang Blow & Stroke\",\"Confessions of a Shopaholic\",\"Post Grad\",\"Cop Out\",\"Our Family Wedding\",\"Easy A\",\"Cats & Dogs: The Revenge of Kitty Galore\",\"Presidential Reunion\",\"The Smurfs\",\"The Smurfs: A Christmas Carol\",\"The Dictator\", \"Fun World\",\"The Smurfs 2\",\"The Smurfs: The Legend of Smurfy Hollow\",\"Salad Days\", \"Addicted to Fresno\", \"Staten Island Summer\",\"Looney Tunes: Rabbits Run\",\"The Damned: Don't You Wish That We Were Dead\",\"Zoolander 2\",\"Ordinary World\",\"The Little Hours\",\"Take the 10\",\"Band Aid\",\"The House of Tomorrow\",\"Battle of the Sexes\",\"The Lego Ninjago Movie\",\"Zane's Stand Up Promo\",\"Game Over, Man!\",\"Jay and Silent Bob Reboot\",\"All Together Now\",\"How It Ends\",\"Too Late\",\"The Mitchells vs. the Machines\",\"Spin Me Round\",\"The Bubble\",\"Untitled Mario film\"]\n",
    "Fred_Armisten_tv = tv\n",
    "Fred_Armisten_dict = {'Films':Fred_Armisten_films,'TV':Fred_Armisten_tv}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2567bb9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Films': ['Guide to Music and South by Southwest',\n",
       "  \"Fred Armisen's Guide to Dance and Self-Defense\",\n",
       "  'I Am Trying to Break Your Heart: A Film About Wilco',\n",
       "  'Like Mike',\n",
       "  'Frank International Film Festival',\n",
       "  'Melvin Goes to Dinner',\n",
       "  'Anchorman: The Legend of Ron Burgundy',\n",
       "  'Eurotrip',\n",
       "  'Wake Up, Ron Burgundy: The Lost Movie',\n",
       "  'Deuce Bigalow: European Gigolo',\n",
       "  'Deck the Halls',\n",
       "  'The Ex',\n",
       "  'Griffin & Phoenix',\n",
       "  'Kiss Me Again',\n",
       "  'Tenacious D in The Pick of Destiny',\n",
       "  'Aqua Teen Hunger Force Colon Movie Film for Theaters',\n",
       "  'Baby Mama',\n",
       "  'Christmas on Mars',\n",
       "  'The Promotion',\n",
       "  'The Rocker',\n",
       "  'Bang Blow & Stroke',\n",
       "  'Confessions of a Shopaholic',\n",
       "  'Post Grad',\n",
       "  'Cop Out',\n",
       "  'Our Family Wedding',\n",
       "  'Easy A',\n",
       "  'Cats & Dogs: The Revenge of Kitty Galore',\n",
       "  'Presidential Reunion',\n",
       "  'The Smurfs',\n",
       "  'The Smurfs: A Christmas Carol',\n",
       "  'The Dictator',\n",
       "  'Fun World',\n",
       "  'The Smurfs 2',\n",
       "  'The Smurfs: The Legend of Smurfy Hollow',\n",
       "  'Salad Days',\n",
       "  'Addicted to Fresno',\n",
       "  'Staten Island Summer',\n",
       "  'Looney Tunes: Rabbits Run',\n",
       "  \"The Damned: Don't You Wish That We Were Dead\",\n",
       "  'Zoolander 2',\n",
       "  'Ordinary World',\n",
       "  'The Little Hours',\n",
       "  'Take the 10',\n",
       "  'Band Aid',\n",
       "  'The House of Tomorrow',\n",
       "  'Battle of the Sexes',\n",
       "  'The Lego Ninjago Movie',\n",
       "  \"Zane's Stand Up Promo\",\n",
       "  'Game Over, Man!',\n",
       "  'Jay and Silent Bob Reboot',\n",
       "  'All Together Now',\n",
       "  'How It Ends',\n",
       "  'Too Late',\n",
       "  'The Mitchells vs. the Machines',\n",
       "  'Spin Me Round',\n",
       "  'The Bubble',\n",
       "  'Untitled Mario film'],\n",
       " 'TV': ['Reverb',\n",
       "  'Fred',\n",
       "  'Late Friday',\n",
       "  'Premium Blend',\n",
       "  'Next!',\n",
       "  'Late World with Zach',\n",
       "  'Saturday Night Live',\n",
       "  'Crank Yankers',\n",
       "  'Comedy Lab',\n",
       "  'Aqua Teen Hunger Force',\n",
       "  'New York Noise',\n",
       "  'Squidbillies',\n",
       "  'Freak Show',\n",
       "  'Night of Too Many Stars',\n",
       "  'Tom Goes to the Mayor',\n",
       "  '30 Rock',\n",
       "  'Tim and Eric Nite Live!',\n",
       "  'Human Giant',\n",
       "  'Yo Gabba Gabba!',\n",
       "  'The Sarah Silverman Program',\n",
       "  'Blue Man Group: How to Be a Megastar 2.0',\n",
       "  'Saturday Night Live Weekend Update Thursday',\n",
       "  'Tim and Eric Awesome Show, Great Job!',\n",
       "  'Parks and Recreation',\n",
       "  'Ugly Americans',\n",
       "  'Portlandia',\n",
       "  'The Looney Tunes Show',\n",
       "  'The Soup',\n",
       "  'Up All Night',\n",
       "  'Late Night with Jimmy Fallon',\n",
       "  'The Simpsons',\n",
       "  'Unsupervised',\n",
       "  \"Bob's Burgers\",\n",
       "  'Conan',\n",
       "  'Kroll Show',\n",
       "  'Jimmy Kimmel Live!',\n",
       "  'Out There',\n",
       "  'The Awesomes',\n",
       "  'Brooklyn Nine-Nine',\n",
       "  'Super Fun Night',\n",
       "  'Broad City',\n",
       "  'Late Night with Seth Meyers',\n",
       "  'House of Lies',\n",
       "  'Chozen',\n",
       "  'Modern Family',\n",
       "  'Comedy Bang! Bang!',\n",
       "  'Comedians in Cars Getting Coffee',\n",
       "  'Archer',\n",
       "  'Mike Tyson Mysteries',\n",
       "  \"Elf: Buddy's Musical Christmas\",\n",
       "  '30th Independent Spirit Awards',\n",
       "  'Man Seeking Woman',\n",
       "  '7 Days in Hell',\n",
       "  'The Jim Gaffigan Show',\n",
       "  'Difficult People',\n",
       "  'Documentary Now!',\n",
       "  'Robot Chicken',\n",
       "  'New Girl',\n",
       "  'Unbreakable Kimmy Schmidt',\n",
       "  'Man Seeking Woman',\n",
       "  'Blunt Talk',\n",
       "  \"Michael Bolton's Big, Sexy Valentine's Day Special\",\n",
       "  'Nature Cat',\n",
       "  'Son of Zorn',\n",
       "  'Animals.',\n",
       "  'Comrade Detective',\n",
       "  'Big Mouth',\n",
       "  'Lady Dynamite',\n",
       "  'The Last Man on Earth',\n",
       "  'I Love You, America with Sarah Silverman',\n",
       "  'A Christmas Story Live!',\n",
       "  'Final Space',\n",
       "  'Splitting Up Together',\n",
       "  'Forever',\n",
       "  'Fortune Rookie',\n",
       "  'Crazy Ex-Girlfriend',\n",
       "  'At Home with Amy Sedaris',\n",
       "  'Los Espookys',\n",
       "  'Superstore',\n",
       "  'The Kacey Musgraves Christmas Show',\n",
       "  'Curb Your Enthusiasm',\n",
       "  'Miracle Workers',\n",
       "  'Central Park',\n",
       "  'Elena of Avalor',\n",
       "  'Mapleworth Murders[2]',\n",
       "  'Moonbase 8',\n",
       "  \"Sarah Cooper: Everything's Fine\",\n",
       "  'Shrill',\n",
       "  'Bless the Harts',\n",
       "  'Schmigadoon!',\n",
       "  'Centaurworld',\n",
       "  'Toast of Tinseltown',\n",
       "  'The Kids in the Hall']}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Fred_Armisten_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "3d751c8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "# get the response in the form of html\n",
    "wikiurl=\"https://en.wikipedia.org/wiki/Aristotle_Athari\"\n",
    "table_class=\"wikitable sortable jquery-tablesorter\"\n",
    "response=requests.get(wikiurl)\n",
    "print(response.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "31eb56a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "indiatable=soup.find('table',{'class':\"wikitable\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ef95e71a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hanging in Hedo', 'Funny Man']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_html(str(indiatable))\n",
    "# convert list to dataframe\n",
    "df=pd.DataFrame(df[0])\n",
    "Aristotle_Athari_films = []\n",
    "for title in df['Title']:\n",
    "    Aristotle_Athari_films.append(title)\n",
    "Aristotle_Athari_films\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "20875e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "indiatable=soup.find_all('table',{'class':\"wikitable\"})[1]\n",
    "                              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "71376cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_html(str(indiatable))\n",
    "# convert list to dataframe\n",
    "df=pd.DataFrame(df[0])\n",
    "Aristotle_Athari_tv = []\n",
    "for title in df['Title']:\n",
    "    Aristotle_Athari_tv.append(title)\n",
    "Aristotle_Athari_tv\n",
    "Aristotle_Athari_dict = {'Film':Aristotle_Athari_films,'TV':Aristotle_Athari_tv}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "9d191eb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Film': ['Hanging in Hedo', 'Funny Man'],\n",
       " 'TV': ['The Coop', 'Silicon Valley', 'Saturday Night Live']}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Aristotle_Athari_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8af1ecae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Love at First Sight',\n",
       " \"Mr. Mike's Mondo Video\",\n",
       " '1941',\n",
       " 'The Blues Brothers',\n",
       " 'Neighbors',\n",
       " 'It Came from Hollywood',\n",
       " 'Doctor Detroit',\n",
       " 'Trading Places',\n",
       " 'Twilight Zone: The Movie',\n",
       " 'Ghostbusters',\n",
       " 'Indiana Jones and the Temple of Doom',\n",
       " 'Nothing Lasts Forever',\n",
       " 'Into the Night',\n",
       " 'Spies Like Us',\n",
       " 'Dragnet',\n",
       " 'The Couch Trip',\n",
       " 'The Great Outdoors',\n",
       " 'Caddyshack II',\n",
       " 'My Stepmother Is an Alien',\n",
       " 'Driving Miss Daisy',\n",
       " 'Ghostbusters II',\n",
       " 'Loose Cannons',\n",
       " 'Masters of Menace',\n",
       " 'My Girl',\n",
       " 'Nothing but Trouble',\n",
       " 'Chaplin',\n",
       " 'Sneakers',\n",
       " 'This Is My Life',\n",
       " 'Coneheads',\n",
       " 'A Century of Cinema',\n",
       " 'Exit to Eden',\n",
       " 'My Girl 2',\n",
       " 'North',\n",
       " 'Canadian Bacon',\n",
       " 'Casper',\n",
       " 'The Random Factor',\n",
       " 'Tommy Boy',\n",
       " 'Rainbow',\n",
       " 'Celtic Pride',\n",
       " 'Feeling Minnesota',\n",
       " 'My Fellow Americans',\n",
       " 'Getting Away with Murder',\n",
       " 'Sgt. Bilko',\n",
       " 'Grosse Pointe Blank',\n",
       " 'Antz',\n",
       " 'Blues Brothers 2000',\n",
       " \"Susan's Plan\",\n",
       " 'Diamonds',\n",
       " 'The House of Mirth',\n",
       " 'Loser',\n",
       " 'Stardom',\n",
       " 'The Curse of the Jade Scorpion',\n",
       " 'Evolution',\n",
       " 'The Frank Truth',\n",
       " 'On the Nose',\n",
       " 'Pearl Harbor',\n",
       " 'Crossroads',\n",
       " 'Unconditional Love',\n",
       " 'Bright Young Things',\n",
       " 'Christmas with the Kranks',\n",
       " '50 First Dates',\n",
       " 'Intern Academy',\n",
       " 'I Now Pronounce You Chuck & Larry',\n",
       " 'Shortcut to Happiness',\n",
       " 'War, Inc.',\n",
       " 'Yogi Bear',\n",
       " 'The Campaign',\n",
       " 'The Ultimate Sacrifice',\n",
       " 'Behind the Candelabra',\n",
       " \"Legends of Oz: Dorothy's Return\",\n",
       " 'Tammy',\n",
       " 'Get on Up',\n",
       " 'Pixels',\n",
       " 'Ghostbusters',\n",
       " \"Cleanin' Up the Town: Remembering Ghostbusters\",\n",
       " 'Ghostbusters: Afterlife']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the response in the form of html\n",
    "wikiurl=\"https://en.wikipedia.org/wiki/Dan_Aykroyd#Filmography\"\n",
    "table_class=\"wikitable sortable jquery-tablesorter\"\n",
    "response=requests.get(wikiurl)\n",
    "print(response.status_code)\n",
    "\n",
    "# parse data from the html into a beautifulsoup object\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "indiatable=soup.find('table',{'class':\"wikitable sortable\"})\n",
    "\n",
    "df=pd.read_html(str(indiatable))\n",
    "# convert list to dataframe\n",
    "df=pd.DataFrame(df[0])\n",
    "Dan_Aykroyd_film = []\n",
    "for title in df['Title']:\n",
    "    Dan_Aykroyd_film.append(title)\n",
    "Dan_Aykroyd_film\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "f93e1d79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['The Gift of Winter',\n",
       " 'Coming Up Rosie',\n",
       " 'Saturday Night Live',\n",
       " \"The Beach Boys: It's OK\",\n",
       " 'All You Need Is Cash',\n",
       " 'The Real Ghostbusters',\n",
       " 'The Dave Thomas Comedy Show',\n",
       " \"It's Garry Shandling's Show\",\n",
       " 'The Earth Day Special',\n",
       " 'Tales from the Crypt',\n",
       " 'The Nanny',\n",
       " 'Kelsey Grammer Salutes Jack Benny',\n",
       " 'Psi Factor: Chronicles of the Paranormal',\n",
       " 'The Arrow',\n",
       " 'Home Improvement',\n",
       " 'Soul Man',\n",
       " 'Normal, Ohio',\n",
       " 'Earth vs. the Spider',\n",
       " \"History's Mysteries\",\n",
       " 'According to Jim',\n",
       " 'Living with Fran',\n",
       " 'Family Guy',\n",
       " 'The Defenders',\n",
       " 'Happily Divorced',\n",
       " \"Workin' Moms\",\n",
       " 'The Conners',\n",
       " 'Hotel Paranormal',\n",
       " 'The Simpsons']"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the response in the form of html\n",
    "wikiurl=\"https://en.wikipedia.org/wiki/Dan_Aykroyd#Filmography\"\n",
    "table_class=\"wikitable sortable jquery-tablesorter\"\n",
    "response=requests.get(wikiurl)\n",
    "print(response.status_code)\n",
    "\n",
    "# parse data from the html into a beautifulsoup object\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "indiatable=soup.find_all('table',{'class':\"wikitable\"})[1]\n",
    "\n",
    "df=pd.read_html(str(indiatable))\n",
    "# convert list to dataframe\n",
    "df=pd.DataFrame(df[0])\n",
    "Dan_Aykroyd_tv = []\n",
    "for title in df['Title']:\n",
    "    Dan_Aykroyd_tv.append(title)\n",
    "Dan_Aykroyd_tv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "b62a3aaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Film': ['Love at First Sight',\n",
       "  \"Mr. Mike's Mondo Video\",\n",
       "  '1941',\n",
       "  'The Blues Brothers',\n",
       "  'Neighbors',\n",
       "  'It Came from Hollywood',\n",
       "  'Doctor Detroit',\n",
       "  'Trading Places',\n",
       "  'Twilight Zone: The Movie',\n",
       "  'Ghostbusters',\n",
       "  'Indiana Jones and the Temple of Doom',\n",
       "  'Nothing Lasts Forever',\n",
       "  'Into the Night',\n",
       "  'Spies Like Us',\n",
       "  'Dragnet',\n",
       "  'The Couch Trip',\n",
       "  'The Great Outdoors',\n",
       "  'Caddyshack II',\n",
       "  'My Stepmother Is an Alien',\n",
       "  'Driving Miss Daisy',\n",
       "  'Ghostbusters II',\n",
       "  'Loose Cannons',\n",
       "  'Masters of Menace',\n",
       "  'My Girl',\n",
       "  'Nothing but Trouble',\n",
       "  'Chaplin',\n",
       "  'Sneakers',\n",
       "  'This Is My Life',\n",
       "  'Coneheads',\n",
       "  'A Century of Cinema',\n",
       "  'Exit to Eden',\n",
       "  'My Girl 2',\n",
       "  'North',\n",
       "  'Canadian Bacon',\n",
       "  'Casper',\n",
       "  'The Random Factor',\n",
       "  'Tommy Boy',\n",
       "  'Rainbow',\n",
       "  'Celtic Pride',\n",
       "  'Feeling Minnesota',\n",
       "  'My Fellow Americans',\n",
       "  'Getting Away with Murder',\n",
       "  'Sgt. Bilko',\n",
       "  'Grosse Pointe Blank',\n",
       "  'Antz',\n",
       "  'Blues Brothers 2000',\n",
       "  \"Susan's Plan\",\n",
       "  'Diamonds',\n",
       "  'The House of Mirth',\n",
       "  'Loser',\n",
       "  'Stardom',\n",
       "  'The Curse of the Jade Scorpion',\n",
       "  'Evolution',\n",
       "  'The Frank Truth',\n",
       "  'On the Nose',\n",
       "  'Pearl Harbor',\n",
       "  'Crossroads',\n",
       "  'Unconditional Love',\n",
       "  'Bright Young Things',\n",
       "  'Christmas with the Kranks',\n",
       "  '50 First Dates',\n",
       "  'Intern Academy',\n",
       "  'I Now Pronounce You Chuck & Larry',\n",
       "  'Shortcut to Happiness',\n",
       "  'War, Inc.',\n",
       "  'Yogi Bear',\n",
       "  'The Campaign',\n",
       "  'The Ultimate Sacrifice',\n",
       "  'Behind the Candelabra',\n",
       "  \"Legends of Oz: Dorothy's Return\",\n",
       "  'Tammy',\n",
       "  'Get on Up',\n",
       "  'Pixels',\n",
       "  'Ghostbusters',\n",
       "  \"Cleanin' Up the Town: Remembering Ghostbusters\",\n",
       "  'Ghostbusters: Afterlife'],\n",
       " 'TV': ['The Gift of Winter',\n",
       "  'Coming Up Rosie',\n",
       "  'Saturday Night Live',\n",
       "  \"The Beach Boys: It's OK\",\n",
       "  'All You Need Is Cash',\n",
       "  'The Real Ghostbusters',\n",
       "  'The Dave Thomas Comedy Show',\n",
       "  \"It's Garry Shandling's Show\",\n",
       "  'The Earth Day Special',\n",
       "  'Tales from the Crypt',\n",
       "  'The Nanny',\n",
       "  'Kelsey Grammer Salutes Jack Benny',\n",
       "  'Psi Factor: Chronicles of the Paranormal',\n",
       "  'The Arrow',\n",
       "  'Home Improvement',\n",
       "  'Soul Man',\n",
       "  'Normal, Ohio',\n",
       "  'Earth vs. the Spider',\n",
       "  \"History's Mysteries\",\n",
       "  'According to Jim',\n",
       "  'Living with Fran',\n",
       "  'Family Guy',\n",
       "  'The Defenders',\n",
       "  'Happily Divorced',\n",
       "  \"Workin' Moms\",\n",
       "  'The Conners',\n",
       "  'Hotel Paranormal',\n",
       "  'The Simpsons']}"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dan_Aykroyd_dict = {'Film':Dan_Aykroyd_film,'TV':Dan_Aykroyd_tv}\n",
    "Dan_Aykroyd_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "7eaf4839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Film': ['Java Junkie',\n",
       "  'Gas',\n",
       "  'Doctor Detroit',\n",
       "  'The Funny Farm',\n",
       "  'Nothing Lasts Forever',\n",
       "  'Spies Like Us',\n",
       "  'Dragnet',\n",
       "  'Nothing but Trouble',\n",
       "  'Coneheads',\n",
       "  'Kids of the Round Table'],\n",
       " 'TV': ['Second City TV',\n",
       "  'Saturday Night Live',\n",
       "  'From Here to Maternity',\n",
       "  'Leo & Liz in Beverly Hills',\n",
       "  'Psi Factor: Chronicles of the Paranormal',\n",
       "  'Justice']}"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the response in the form of html\n",
    "wikiurl=\"https://en.wikipedia.org/wiki/Peter_Aykroyd#Filmography\"\n",
    "table_class=\"wikitable\"\n",
    "response=requests.get(wikiurl)\n",
    "print(response.status_code)\n",
    "\n",
    "# parse data from the html into a beautifulsoup object\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "indiatable=soup.find_all('table',{'class':\"wikitable\"})[0]\n",
    "\n",
    "df=pd.read_html(str(indiatable))\n",
    "# convert list to dataframe\n",
    "df=pd.DataFrame(df[0])\n",
    "Peter_Aykroyd_film = []\n",
    "for title in df['Title']:\n",
    "    Peter_Aykroyd_film.append(title)\n",
    "Peter_Aykroyd_film\n",
    "\n",
    "# parse data from the html into a beautifulsoup object\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "indiatable=soup.find_all('table',{'class':\"wikitable\"})[1]\n",
    "\n",
    "df=pd.read_html(str(indiatable))\n",
    "# convert list to dataframe\n",
    "df=pd.DataFrame(df[0])\n",
    "Peter_Aykroyd_tv = []\n",
    "for title in df['Title']:\n",
    "    Peter_Aykroyd_tv.append(title)\n",
    "Peter_Aykroyd_tv\n",
    "\n",
    "Peter_Aykroyd_dict = {'Film':Peter_Aykroyd_film,'TV':Peter_Aykroyd_tv}\n",
    "Peter_Aykroyd_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "8e24cc84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Film': ['Off the Cuff',\n",
       "  'Stages of Emily',\n",
       "  'Adventures in the Sin Bin',\n",
       "  'Despicable Me 2',\n",
       "  'Trainwreck',\n",
       "  'Carrie Pilby',\n",
       "  'Office Christmas Party',\n",
       "  'The Polka King',\n",
       "  'Ibiza',\n",
       "  'Wander Darkly',\n",
       "  'Barb and Star Go to Vista Del Mar',\n",
       "  'DC League of Super-Pets'],\n",
       " 'TV': ['Saturday Night Live',\n",
       "  'I Wanna Have Your Baby',\n",
       "  'Sugarboy',\n",
       "  'The Mindy Project',\n",
       "  'Comedy Bang! Bang!',\n",
       "  'Wallykazam!',\n",
       "  'Portlandia',\n",
       "  'Man Seeking Woman',\n",
       "  'The Awesomes',\n",
       "  'Modern Family',\n",
       "  'Drunk History',\n",
       "  'The Simpsons',\n",
       "  'Crashing',\n",
       "  'Love',\n",
       "  'Will & Grace',\n",
       "  'Single Parents',\n",
       "  'What We Do in the Shadows',\n",
       "  'I Think You Should Leave with Tim Robinson',\n",
       "  'Trolls: The Beat Goes On!',\n",
       "  'Helpsters',\n",
       "  'Shrill',\n",
       "  'Brooklyn Nine-Nine',\n",
       "  \"Alice's Wonderland Bakery\",\n",
       "  'I Love That for You']}"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the response in the form of html\n",
    "wikiurl=\"https://en.wikipedia.org/wiki/Vanessa_Bayer#Filmography\"\n",
    "table_class=\"wikitable\"\n",
    "response=requests.get(wikiurl)\n",
    "print(response.status_code)\n",
    "\n",
    "# parse data from the html into a beautifulsoup object\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "indiatable=soup.find_all('table',{'class':\"wikitable\"})[0]\n",
    "\n",
    "df=pd.read_html(str(indiatable))\n",
    "# convert list to dataframe\n",
    "df=pd.DataFrame(df[0])\n",
    "Vanessa_Bayer_film = []\n",
    "for title in df['Title']:\n",
    "    Vanessa_Bayer_film.append(title)\n",
    "Vanessa_Bayer_film\n",
    "\n",
    "# parse data from the html into a beautifulsoup object\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "indiatable=soup.find_all('table',{'class':\"wikitable\"})[1]\n",
    "\n",
    "df=pd.read_html(str(indiatable))\n",
    "# convert list to dataframe\n",
    "df=pd.DataFrame(df[0])\n",
    "Vanessa_Bayer_tv = []\n",
    "for title in df['Title']:\n",
    "    Vanessa_Bayer_tv.append(title)\n",
    "Vanessa_Bayer_tv\n",
    "\n",
    "Vanessa_Bayer_dict = {'Film':Vanessa_Bayer_film,'TV':Vanessa_Bayer_tv}\n",
    "Vanessa_Bayer_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "2de9a66d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Film': ['The Fury',\n",
       "  'Thief',\n",
       "  'Trading Places',\n",
       "  'The Man with One Red Shoe',\n",
       "  'Little Shop of Horrors',\n",
       "  'Salvador',\n",
       "  \"Jumpin' Jack Flash\",\n",
       "  'About Last Night...',\n",
       "  'The Principal',\n",
       "  'Real Men',\n",
       "  'Red Heat',\n",
       "  'K-9',\n",
       "  'Homer and Eddie',\n",
       "  \"Who's Harry Crumb?\",\n",
       "  'Taking Care of Business',\n",
       "  'Mr. Destiny',\n",
       "  'Masters of Menace',\n",
       "  'Dimenticare Palermo',\n",
       "  'Wedding Band',\n",
       "  'Curly Sue',\n",
       "  'Diary of a Hitman',\n",
       "  'Only the Lonely',\n",
       "  'Abraxas, Guardian of the Universe',\n",
       "  'Traces of Red',\n",
       "  'Once Upon a Crime',\n",
       "  'Last Action Hero',\n",
       "  'Canadian Bacon',\n",
       "  'Separate Lives',\n",
       "  'Sahara',\n",
       "  'The Pebble and the Penguin',\n",
       "  'Destiny Turns on the Radio',\n",
       "  'Irving',\n",
       "  'Jingle All the Way',\n",
       "  'Race the Sun',\n",
       "  'Gold in the Streets',\n",
       "  'Gang Related',\n",
       "  'Retroactive',\n",
       "  'Living in Peril',\n",
       "  'Bad Baby',\n",
       "  'Wag the Dog',\n",
       "  'Babes in Toyland',\n",
       "  'Overnight Delivery',\n",
       "  \"Angel's Dance\",\n",
       "  'Made Men',\n",
       "  'K-911',\n",
       "  'The Florentine',\n",
       "  'The Nuttiest Nutcracker',\n",
       "  'Return to Me',\n",
       "  'Joe Somebody',\n",
       "  'Snow Dogs',\n",
       "  'Pinocchio',\n",
       "  'One Way Out',\n",
       "  'K-9: P.I.',\n",
       "  'Easy Six',\n",
       "  'DysEnchanted',\n",
       "  'Hoodwinked!',\n",
       "  'My Neighbors the Yamadas',\n",
       "  'Tugger: The Jeep 4x4 Who Wanted to Fly',\n",
       "  'The Wild',\n",
       "  'Underdog',\n",
       "  'Once Upon a Christmas Village',\n",
       "  'Farce of the Penguins',\n",
       "  'Scooby-Doo! and the Goblin King',\n",
       "  'Snow Buddies',\n",
       "  'The Ghost Writer',\n",
       "  'Cougars, Inc.',\n",
       "  \"New Year's Eve\",\n",
       "  'The Secret Lives of Dorks',\n",
       "  'Thunderstruck',\n",
       "  \"Legends of Oz: Dorothy's Return\",\n",
       "  'Home Sweet Hell',\n",
       "  'Undrafted',\n",
       "  'The Whole Truth',\n",
       "  'The Hollow Point',\n",
       "  'Katie Says Goodbye',\n",
       "  'A Change of Heart',\n",
       "  'Sollers Point',\n",
       "  'Wonder Wheel'],\n",
       " 'TV': [\"Who's Watching the Kids?\",\n",
       "  'Working Stiffs',\n",
       "  'Laverne & Shirley',\n",
       "  'Saturday Night Live',\n",
       "  'Faerie Tale Theatre',\n",
       "  'The Best Legs in the Eighth Grade',\n",
       "  'The Birthday Boy',\n",
       "  'Wild Palms',\n",
       "  'The Building',\n",
       "  'Royce',\n",
       "  'Parallel Lives',\n",
       "  'Aaahh!!! Real Monsters',\n",
       "  'Sahara',\n",
       "  'Santo Bugito',\n",
       "  'Duckman',\n",
       "  'Pinky and the Brain',\n",
       "  'The Twisted Tales of Felix the Cat',\n",
       "  'Gargoyles',\n",
       "  'Timon & Pumbaa',\n",
       "  'The Tick',\n",
       "  'KaBlam!',\n",
       "  'Mighty Ducks',\n",
       "  'Hey Arnold!',\n",
       "  'Total Security',\n",
       "  \"Dog's Best Friend\",\n",
       "  'Cow and Chicken',\n",
       "  'Life with Louie',\n",
       "  'The Blues Brothers: The Animated Series',\n",
       "  'The Larry Sanders Show',\n",
       "  'Hercules',\n",
       "  'Stories from My Childhood',\n",
       "  'Justice',\n",
       "  'Hooves of Fire',\n",
       "  \"Who Killed Atlanta's Children?\",\n",
       "  'Beggars and Choosers',\n",
       "  'ER',\n",
       "  'According to Jim',\n",
       "  'Rugrats',\n",
       "  \"What's New, Scooby-Doo?\",\n",
       "  'The Adventures of Jimmy Neutron: Boy Genius',\n",
       "  'Ozzy & Drix',\n",
       "  \"I'm with Her\",\n",
       "  'Less than Perfect',\n",
       "  'George Lopez',\n",
       "  'Fatherhood',\n",
       "  \"Casper's Scare School\",\n",
       "  'Handy Manny',\n",
       "  'The Defenders',\n",
       "  'Doc McStuffins',\n",
       "  \"Stan Lee's Mighty 7: Beginnings\",\n",
       "  'Show Me a Hero',\n",
       "  'Building Belushi',\n",
       "  'Urban Cowboy',\n",
       "  'TripTank',\n",
       "  'Good Girls Revolt',\n",
       "  'The 7D',\n",
       "  'Mating',\n",
       "  'Twin Peaks',\n",
       "  'Hey Arnold!: The Jungle Movie',\n",
       "  'Salvage',\n",
       "  'Trolls: The Beat Goes On!',\n",
       "  'Growing Belushi']}"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the response in the form of html\n",
    "wikiurl=\"https://en.wikipedia.org/wiki/Jim_Belushi#Film\"\n",
    "table_class=\"wikitable\"\n",
    "response=requests.get(wikiurl)\n",
    "print(response.status_code)\n",
    "\n",
    "# parse data from the html into a beautifulsoup object\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "indiatable=soup.find_all('table',{'class':\"wikitable\"})[0]\n",
    "\n",
    "df=pd.read_html(str(indiatable))\n",
    "# convert list to dataframe\n",
    "df=pd.DataFrame(df[0])\n",
    "Jim_Belushi_film = []\n",
    "for title in df['Title']:\n",
    "    Jim_Belushi_film.append(title)\n",
    "Jim_Belushi_film\n",
    "\n",
    "# parse data from the html into a beautifulsoup object\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "indiatable=soup.find_all('table',{'class':\"wikitable\"})[1]\n",
    "\n",
    "df=pd.read_html(str(indiatable))\n",
    "# convert list to dataframe\n",
    "df=pd.DataFrame(df[0])\n",
    "Jim_Belushi_tv = []\n",
    "for title in df['Title']:\n",
    "    Jim_Belushi_tv.append(title)\n",
    "Jim_Belushi_tv\n",
    "\n",
    "Jim_Belushi_dict = {'Film':Jim_Belushi_film,'TV':Jim_Belushi_tv}\n",
    "Jim_Belushi_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "7494de19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Film': ['Tarzoon: Shame of the Jungle',\n",
       "  'Animal House',\n",
       "  \"Goin' South\",\n",
       "  'Old Boyfriends',\n",
       "  '1941',\n",
       "  'The Blues Brothers',\n",
       "  'Continental Divide',\n",
       "  'Neighbors'],\n",
       " 'TV': ['Saturday Night Live',\n",
       "  \"The Beach Boys: It's OK\",\n",
       "  'The Rutles: All You Need Is Cash']}"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the response in the form of html\n",
    "wikiurl=\"https://en.wikipedia.org/wiki/John_Belushi#Film\"\n",
    "table_class=\"wikitable\"\n",
    "response=requests.get(wikiurl)\n",
    "print(response.status_code)\n",
    "\n",
    "# parse data from the html into a beautifulsoup object\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "indiatable=soup.find_all('table',{'class':\"wikitable\"})[0]\n",
    "\n",
    "df=pd.read_html(str(indiatable))\n",
    "# convert list to dataframe\n",
    "df=pd.DataFrame(df[0])\n",
    "John_Belushi_film = []\n",
    "for title in df['Title']:\n",
    "    John_Belushi_film.append(title)\n",
    "John_Belushi_film\n",
    "\n",
    "# parse data from the html into a beautifulsoup object\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "indiatable=soup.find_all('table',{'class':\"wikitable\"})[1]\n",
    "\n",
    "df=pd.read_html(str(indiatable))\n",
    "# convert list to dataframe\n",
    "df=pd.DataFrame(df[0])\n",
    "John_Belushi_tv = []\n",
    "for title in df['Title']:\n",
    "    John_Belushi_tv.append(title)\n",
    "John_Belushi_tv\n",
    "\n",
    "John_Belushi_dict = {'Film':John_Belushi_film,'TV':John_Belushi_tv}\n",
    "John_Belushi_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "f05bbd51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Film': ['Kill Me Now',\n",
       "  'Beside Still Waters',\n",
       "  'Balls Out',\n",
       "  'The Party is Over',\n",
       "  'Zoolander 2',\n",
       "  'Dean',\n",
       "  'Sing',\n",
       "  'The Late Bloomer',\n",
       "  'Brigsby Bear',\n",
       "  'The Unicorn',\n",
       "  'Greener Grass',\n",
       "  'Plus One',\n",
       "  'The Angry Birds Movie 2',\n",
       "  'Bill & Ted Face the Music',\n",
       "  'The Mitchells vs. the Machines'],\n",
       " 'TV': ['Last Man Standing',\n",
       "  'Arrested Development',\n",
       "  'Axe Cop',\n",
       "  'Lucas Bros. Moving Co',\n",
       "  'Saturday Night Live',\n",
       "  'Big Time in Hollywood, FL',\n",
       "  'Master of None',\n",
       "  'Comrade Detective',\n",
       "  'Ghosted',\n",
       "  'DuckTales',\n",
       "  'The Other Two',\n",
       "  'Shrill',\n",
       "  'Sunnyside',\n",
       "  'Close Enough',\n",
       "  'M.O.D.O.K.',\n",
       "  'The Simpsons']}"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the response in the form of html\n",
    "wikiurl=\"https://en.wikipedia.org/wiki/Beck_Bennett#Filmography\"\n",
    "table_class=\"wikitable\"\n",
    "response=requests.get(wikiurl)\n",
    "print(response.status_code)\n",
    "\n",
    "# parse data from the html into a beautifulsoup object\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "indiatable=soup.find_all('table',{'class':\"wikitable\"})[0]\n",
    "\n",
    "df=pd.read_html(str(indiatable))\n",
    "# convert list to dataframe\n",
    "df=pd.DataFrame(df[0])\n",
    "Beck_Bennett_film = []\n",
    "for title in df['Title']:\n",
    "    Beck_Bennett_film.append(title)\n",
    "Beck_Bennett_film\n",
    "\n",
    "# parse data from the html into a beautifulsoup object\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "indiatable=soup.find_all('table',{'class':\"wikitable\"})[1]\n",
    "\n",
    "df=pd.read_html(str(indiatable))\n",
    "# convert list to dataframe\n",
    "df=pd.DataFrame(df[0])\n",
    "Beck_Bennett_tv = []\n",
    "for title in df['Title']:\n",
    "    Beck_Bennett_tv.append(title)\n",
    "Beck_Bennett_tv\n",
    "\n",
    "Beck_Bennett_dict = {'Film':Beck_Bennett_film,'TV':Beck_Bennett_tv}\n",
    "Beck_Bennett_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "e6892467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Film': [],\n",
       " 'TV': ['Uptown Comedy Club',\n",
       "  'Saturday Night Live',\n",
       "  'Home Improvement',\n",
       "  'Clerks: The TV Show',\n",
       "  \"'Buddies\",\n",
       "  'Half Baked',\n",
       "  'Dick',\n",
       "  'Titan A.E.',\n",
       "  'Once in the Life',\n",
       "  'One Eyed King',\n",
       "  'American Dummy',\n",
       "  'Crooked Lines',\n",
       "  'Beer League',\n",
       "  'More than Me',\n",
       "  'The Jim Breuer Road Journals',\n",
       "  'Zookeeper',\n",
       "  'Motorcity',\n",
       "  'The English Teacher',\n",
       "  'Family Guy',\n",
       "  'School Dance',\n",
       "  'Quitters',\n",
       "  'Bling',\n",
       "  'Rock and a Hard Place',\n",
       "  'Kevin Can Wait',\n",
       "  'Liv & Maddie']}"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the response in the form of html\n",
    "wikiurl=\"https://en.wikipedia.org/wiki/Jim_Breuer#Filmography\"\n",
    "table_class=\"wikitable\"\n",
    "response=requests.get(wikiurl)\n",
    "print(response.status_code)\n",
    "\n",
    "# parse data from the html into a beautifulsoup object\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "indiatable=soup.find_all('table',{'class':\"wikitable\"})[0]\n",
    "\n",
    "df=pd.read_html(str(indiatable))\n",
    "# convert list to dataframe\n",
    "df=pd.DataFrame(df[0])\n",
    "Jim_Breuer_tv = []\n",
    "for title in df['Title']:\n",
    "    Jim_Breuer_tv.append(title)\n",
    "Jim_Breuer_tv\n",
    "\n",
    "\n",
    "Jim_Breuer_dict = {'Film':[],'TV':Jim_Breuer_tv}\n",
    "Jim_Breuer_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "59b9c9a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Film': ['Hotel Transylvania',\n",
       "  'Grown Ups 2',\n",
       "  'The Night is Young',\n",
       "  'Nigel & Oscar vs. the Sasquatch',\n",
       "  'Killing Gunther'],\n",
       " 'TV': ['Saturday Night Live',\n",
       "  'Hello Ladies',\n",
       "  'Trophy Wife',\n",
       "  'Kroll Show',\n",
       "  'Comedy Bang! Bang!',\n",
       "  'Shrink']}"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the response in the form of html\n",
    "wikiurl=\"https://en.wikipedia.org/wiki/Paul_Brittain#Filmography\"\n",
    "table_class=\"wikitable\"\n",
    "response=requests.get(wikiurl)\n",
    "print(response.status_code)\n",
    "\n",
    "# parse data from the html into a beautifulsoup object\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "indiatable=soup.find_all('table',{'class':\"wikitable\"})[0]\n",
    "\n",
    "df=pd.read_html(str(indiatable))\n",
    "# convert list to dataframe\n",
    "df=pd.DataFrame(df[0])\n",
    "Paul_Brittain_film = []\n",
    "for title in df['Title']:\n",
    "    Paul_Brittain_film.append(title)\n",
    "Paul_Brittain_film\n",
    "\n",
    "# parse data from the html into a beautifulsoup object\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "indiatable=soup.find_all('table',{'class':\"wikitable\"})[1]\n",
    "\n",
    "df=pd.read_html(str(indiatable))\n",
    "# convert list to dataframe\n",
    "df=pd.DataFrame(df[0])\n",
    "Paul_Brittain_tv = []\n",
    "for title in df['Title']:\n",
    "    Paul_Brittain_tv.append(title)\n",
    "Paul_Brittain_tv\n",
    "\n",
    "Paul_Brittain_dict = {'Film':Paul_Brittain_film,'TV':Paul_Brittain_tv}\n",
    "Paul_Brittain_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "1120883b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Film': ['The Amazing Spider-Man 2',\n",
       "  'Kids',\n",
       "  'Prom Queen',\n",
       "  'Brother Nature',\n",
       "  'Darby Forever',\n",
       "  'The Big Sick',\n",
       "  'The Star',\n",
       "  'I Feel Pretty'],\n",
       " 'TV': ['Shrink',\n",
       "  'Saturday Night Live',\n",
       "  'Saturday Night Live Weekend Update Thursday',\n",
       "  'Comedy Bang! Bang!',\n",
       "  'The Greatest Event in Television History',\n",
       "  'Broad City',\n",
       "  'Documentary Now!',\n",
       "  'The Awesomes',\n",
       "  'Girls',\n",
       "  'Horace and Pete',\n",
       "  'Danger & Eggs',\n",
       "  'At Home with Amy Sedaris',\n",
       "  'Portlandia',\n",
       "  'Unbreakable Kimmy Schmidt',\n",
       "  'The Other Two',\n",
       "  'Shrill',\n",
       "  'Human Resources']}"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the response in the form of html\n",
    "wikiurl=\"https://en.wikipedia.org/wiki/Aidy_Bryant#Filmography\"\n",
    "table_class=\"wikitable\"\n",
    "response=requests.get(wikiurl)\n",
    "print(response.status_code)\n",
    "\n",
    "# parse data from the html into a beautifulsoup object\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "indiatable=soup.find_all('table',{'class':\"wikitable\"})[0]\n",
    "\n",
    "df=pd.read_html(str(indiatable))\n",
    "# convert list to dataframe\n",
    "df=pd.DataFrame(df[0])\n",
    "Aidy_Bryant_film = []\n",
    "for title in df['Title']:\n",
    "    Aidy_Bryant_film.append(title)\n",
    "Aidy_Bryant_film\n",
    "\n",
    "# parse data from the html into a beautifulsoup object\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "indiatable=soup.find_all('table',{'class':\"wikitable\"})[1]\n",
    "\n",
    "df=pd.read_html(str(indiatable))\n",
    "# convert list to dataframe\n",
    "df=pd.DataFrame(df[0])\n",
    "Aidy_Bryant_tv = []\n",
    "for title in df['Title']:\n",
    "    Aidy_Bryant_tv.append(title)\n",
    "Aidy_Bryant_tv\n",
    "\n",
    "Aidy_Bryant_dict = {'Film':Aidy_Bryant_film,'TV':Aidy_Bryant_tv}\n",
    "Aidy_Bryant_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "db39f2db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Film': ['Halloween II',\n",
       "  'This Is Spinal Tap',\n",
       "  'Racing with the Moon',\n",
       "  'Tough Guys',\n",
       "  'Moving',\n",
       "  'Opportunity Knocks',\n",
       "  \"Wayne's World\",\n",
       "  \"Wayne's World 2\",\n",
       "  'Clean Slate',\n",
       "  'The Road to Wellville',\n",
       "  'Trapped in Paradise',\n",
       "  'The Shot',\n",
       "  'Fire on the Track: The Steve Prefontaine Story',\n",
       "  'Little Nicky',\n",
       "  'The Master of Disguise',\n",
       "  'Presidential Reunion',\n",
       "  'Jack and Jill',\n",
       "  'Hotel Transylvania 2',\n",
       "  'The Secret Life of Pets',\n",
       "  'Sandy Wexler',\n",
       "  'Becoming Bond',\n",
       "  'Too Funny to Fail',\n",
       "  'The Secret Life of Pets 2'],\n",
       " 'TV': ['One of the Boys',\n",
       "  'Blue Thunder',\n",
       "  'Saturday Night Live',\n",
       "  'The Larry Sanders Show',\n",
       "  'Saturday Night Live',\n",
       "  \"Dana Carvey: Critics' Choice\",\n",
       "  'The Dana Carvey Show',\n",
       "  'Just Shoot Me!',\n",
       "  'LateLine',\n",
       "  'Dana Carvey: Squatting Monkeys Tell No Lies',\n",
       "  'The Fairly OddParents',\n",
       "  'Good Vibes',\n",
       "  'Spoof',\n",
       "  'Live with Kelly',\n",
       "  'Rick and Morty',\n",
       "  'The Birthday Boys',\n",
       "  'First Impressions',\n",
       "  'Dana Carvey: Straight White Male, 60',\n",
       "  'Comedians in Cars Getting Coffee',\n",
       "  'Bajillion Dollar Propertie$']}"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the response in the form of html\n",
    "wikiurl=\"https://en.wikipedia.org/wiki/Dana_Carvey#Filmography\"\n",
    "table_class=\"wikitable\"\n",
    "response=requests.get(wikiurl)\n",
    "print(response.status_code)\n",
    "\n",
    "# parse data from the html into a beautifulsoup object\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "indiatable=soup.find_all('table',{'class':\"wikitable\"})[0]\n",
    "\n",
    "df=pd.read_html(str(indiatable))\n",
    "# convert list to dataframe\n",
    "df=pd.DataFrame(df[0])\n",
    "Dana_Carvey_film = []\n",
    "for title in df['Title']:\n",
    "    Dana_Carvey_film.append(title)\n",
    "Dana_Carvey_film\n",
    "\n",
    "# parse data from the html into a beautifulsoup object\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "indiatable=soup.find_all('table',{'class':\"wikitable\"})[1]\n",
    "\n",
    "df=pd.read_html(str(indiatable))\n",
    "# convert list to dataframe\n",
    "df=pd.DataFrame(df[0])\n",
    "Dana_Carvey_tv = []\n",
    "for title in df['Title']:\n",
    "    Dana_Carvey_tv.append(title)\n",
    "Dana_Carvey_tv\n",
    "\n",
    "Dana_Carvey_dict = {'Film':Dana_Carvey_film,'TV':Dana_Carvey_tv}\n",
    "Dana_Carvey_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "6ecfaf24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Film': [\"Walk... Don't Walk\",\n",
       "  'The Groove Tube',\n",
       "  'Tunnel Vision',\n",
       "  'Foul Play',\n",
       "  'Oh! Heavenly Dog',\n",
       "  'Caddyshack',\n",
       "  'Seems Like Old Times',\n",
       "  'Under the Rainbow',\n",
       "  'Modern Problems',\n",
       "  \"National Lampoon's Vacation\",\n",
       "  'Deal of the Century',\n",
       "  'Fletch',\n",
       "  \"National Lampoon's European Vacation\",\n",
       "  'Sesame Street Presents: Follow That Bird',\n",
       "  'Spies Like Us',\n",
       "  'Three Amigos',\n",
       "  'The Couch Trip',\n",
       "  'Funny Farm',\n",
       "  'Caddyshack II',\n",
       "  'Fletch Lives',\n",
       "  \"National Lampoon's Christmas Vacation\",\n",
       "  'Nothing but Trouble',\n",
       "  'L.A. Story',\n",
       "  'Memoirs of an Invisible Man',\n",
       "  'Hero',\n",
       "  'Last Action Hero',\n",
       "  'A Century of Cinema',\n",
       "  'Cops & Robbersons',\n",
       "  'Man of the House',\n",
       "  'Vegas Vacation',\n",
       "  'Dirty Work',\n",
       "  'Snow Day',\n",
       "  \"Pete's Pizza\",\n",
       "  'The One Armed Bandit',\n",
       "  'Orange County',\n",
       "  'Vacuums',\n",
       "  'Bitter Jester',\n",
       "  'Our Italian Husband',\n",
       "  'Bad Meat',\n",
       "  'Ellie Parker',\n",
       "  'Funny Money',\n",
       "  'Doogal',\n",
       "  'Goose on the Loose',\n",
       "  'Zoom',\n",
       "  'Stay Cool',\n",
       "  'Jack and the Beanstalk',\n",
       "  'Hot Tub Time Machine',\n",
       "  'Hotel Hell Vacation',\n",
       "  'Not Another Not Another Movie',\n",
       "  'Before I Sleep',\n",
       "  'Lovesick',\n",
       "  'Shelby',\n",
       "  'Hot Tub Time Machine 2',\n",
       "  'Vacation',\n",
       "  'The Last Movie Star',\n",
       "  'Hedgehogs',\n",
       "  'The Last Laugh',\n",
       "  'The Very Excellent Mr. Dundee',\n",
       "  'Panda vs. Aliens'],\n",
       " 'TV': ['The Smothers Brothers Show',\n",
       "  'Saturday Night Live',\n",
       "  'The Chevy Chase Show',\n",
       "  'The Paul Simon Special',\n",
       "  'The Chevy Chase National Humor Test',\n",
       "  '60th Academy Awards',\n",
       "  'The Earth Day Special',\n",
       "  'The Chevy Chase Show',\n",
       "  'The Larry Sanders Show',\n",
       "  'The Nanny',\n",
       "  \"America's Most Terrible Things\",\n",
       "  'Freedom: A History of US',\n",
       "  'The Karate Dog',\n",
       "  \"The Secret Policeman's Ball\",\n",
       "  'Law & Order',\n",
       "  'Family Guy',\n",
       "  'Brothers & Sisters',\n",
       "  'Hjälp!',\n",
       "  'Chuck',\n",
       "  'Community',\n",
       "  'Hot in Cleveland',\n",
       "  \"Wishin' and Hopin'\",\n",
       "  'Chevy',\n",
       "  'A Christmas in Vermont']}"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the response in the form of html\n",
    "wikiurl=\"https://en.wikipedia.org/wiki/Chevy_Chase#Filmography\"\n",
    "table_class=\"wikitable\"\n",
    "response=requests.get(wikiurl)\n",
    "print(response.status_code)\n",
    "\n",
    "# parse data from the html into a beautifulsoup object\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "indiatable=soup.find_all('table',{'class':\"wikitable\"})[0]\n",
    "\n",
    "df=pd.read_html(str(indiatable))\n",
    "# convert list to dataframe\n",
    "df=pd.DataFrame(df[0])\n",
    "Chevy_Chase_film = []\n",
    "for title in df['Title']:\n",
    "    Chevy_Chase_film.append(title)\n",
    "Chevy_Chase_film\n",
    "\n",
    "# parse data from the html into a beautifulsoup object\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "indiatable=soup.find_all('table',{'class':\"wikitable\"})[1]\n",
    "\n",
    "df=pd.read_html(str(indiatable))\n",
    "# convert list to dataframe\n",
    "df=pd.DataFrame(df[0])\n",
    "Chevy_Chase_tv = []\n",
    "for title in df['Title']:\n",
    "    Chevy_Chase_tv.append(title)\n",
    "Chevy_Chase_tv\n",
    "\n",
    "Chevy_Chase_dict = {'Film':Chevy_Chase_film,'TV':Chevy_Chase_tv}\n",
    "Chevy_Chase_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "469edbef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Film': ['Chinese Puzzle', 'Lyle', 'Top Five'],\n",
       " 'TV': [\"John Oliver's New York Stand-Up Show\",\n",
       "  'Saturday Night Live',\n",
       "  'The Half Hour',\n",
       "  'The Daily Show',\n",
       "  'Michael Che Matters',\n",
       "  'Saturday Night Live Weekend Update Thursday',\n",
       "  '70th Primetime Emmy Awards',\n",
       "  \"Seth Rogen's Hilarity for Charity\",\n",
       "  'The Other Two',\n",
       "  'Sesame Street',\n",
       "  'WWE Raw',\n",
       "  'WrestleMania 35',\n",
       "  'That Damn Michael Che',\n",
       "  'Michael Che: Shame the Devil']}"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the response in the form of html\n",
    "wikiurl=\"https://en.wikipedia.org/wiki/Michael_Che#Filmography\"\n",
    "table_class=\"wikitable\"\n",
    "response=requests.get(wikiurl)\n",
    "print(response.status_code)\n",
    "\n",
    "# parse data from the html into a beautifulsoup object\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "indiatable=soup.find_all('table',{'class':\"wikitable\"})[0]\n",
    "\n",
    "df=pd.read_html(str(indiatable))\n",
    "# convert list to dataframe\n",
    "df=pd.DataFrame(df[0])\n",
    "Michael_Che_film = []\n",
    "for title in df['Film']:\n",
    "    Michael_Che_film.append(title)\n",
    "Michael_Che_film\n",
    "\n",
    "# parse data from the html into a beautifulsoup object\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "indiatable=soup.find_all('table',{'class':\"wikitable\"})[1]\n",
    "\n",
    "df=pd.read_html(str(indiatable))\n",
    "# convert list to dataframe\n",
    "df=pd.DataFrame(df[0])\n",
    "Michael_Che_tv = []\n",
    "for title in df['Series']:\n",
    "    Michael_Che_tv.append(title)\n",
    "Michael_Che_tv\n",
    "\n",
    "Michael_Che_dict = {'Film':Michael_Che_film,'TV':Michael_Che_tv}\n",
    "Michael_Che_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "ab7a8aeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Film': ['De Düva: The Dove',\n",
       "  'Distance',\n",
       "  'The Stepford Wives',\n",
       "  'French Postcards',\n",
       "  'Kramer vs. Kramer',\n",
       "  'The First Deadly Sin',\n",
       "  \"Bustin' Loose\",\n",
       "  'The Amateur',\n",
       "  'The Entity',\n",
       "  'The House of God',\n",
       "  'Micki + Maude',\n",
       "  'Remo Williams: The Adventure Begins',\n",
       "  'Head Office',\n",
       "  'Into the Woods',\n",
       "  'Blind Date',\n",
       "  'Best Seller',\n",
       "  'Cousins',\n",
       "  'The End of Innocence',\n",
       "  'The Mighty Ducks',\n",
       "  'Gospa',\n",
       "  'Nick and Jane',\n",
       "  'The Omega Code',\n",
       "  'Big Eden',\n",
       "  'A Rumor of Angels',\n",
       "  'Diamond Men',\n",
       "  'Corporate Affairs',\n",
       "  'Funny People',\n",
       "  'Slice of Water',\n",
       "  'Bathtub Picnic',\n",
       "  'Transformers: Dark of the Moon',\n",
       "  'Wrinkles',\n",
       "  'Chez Upshaw',\n",
       "  '13 Sins'],\n",
       " 'TV': ['For the People',\n",
       "  'Somerset',\n",
       "  'How to Succeed in Business Without Really Trying',\n",
       "  'The Doctors',\n",
       "  'Saturday Night Live',\n",
       "  'Big Blonde',\n",
       "  'Kent State',\n",
       "  'Broken Promise',\n",
       "  'Red Flag: The Ultimate Game',\n",
       "  \"Dreams Don't Die\",\n",
       "  'Drop-Out Father',\n",
       "  'The Country Girl',\n",
       "  'Listen to Your Heart',\n",
       "  'Rage of Angels',\n",
       "  'Goodnight, Beantown',\n",
       "  'Sessions',\n",
       "  'Hill Street Blues',\n",
       "  \"He's Not Your Son\",\n",
       "  'American Playhouse',\n",
       "  'My Wicked, Wicked Ways: The Legend of Errol Flynn',\n",
       "  'The Insiders',\n",
       "  'Remington Steele',\n",
       "  'Moonlighting',\n",
       "  'Family Ties',\n",
       "  'Blood & Orchids',\n",
       "  'Simon & Simon',\n",
       "  'The Paper Chase',\n",
       "  'Dallas',\n",
       "  'Scarecrow and Mrs. King',\n",
       "  'L.A. Law',\n",
       "  \"Uncle Tom's Cabin\",\n",
       "  'Max Headroom',\n",
       "  'thirtysomething',\n",
       "  'Magnum, P.I.',\n",
       "  'The Golden Girls',\n",
       "  'Shootdown',\n",
       "  'Why on Earth?',\n",
       "  'Those She Left Behind',\n",
       "  'My Name Is Bill W.',\n",
       "  'The Hollywood Detective',\n",
       "  'The Tracey Ullman Show',\n",
       "  'Columbo',\n",
       "  'Matlock',\n",
       "  'Amen',\n",
       "  'Father Dowling Mysteries',\n",
       "  'Fine Things',\n",
       "  'Murphy Brown',\n",
       "  'To My Daughter',\n",
       "  'Star Trek: The Next Generation',\n",
       "  'Equal Justice',\n",
       "  'Night Court',\n",
       "  'Murder, She Wrote',\n",
       "  'Something to Live for: The Alison Gertz Story',\n",
       "  'Nurses',\n",
       "  \"Bobby's World\",\n",
       "  'The Human Factor',\n",
       "  'Crossroads',\n",
       "  'Camp Candy',\n",
       "  'Law & Order',\n",
       "  \"Joe's Life\",\n",
       "  'The Cosby Mysteries',\n",
       "  'New York Undercover',\n",
       "  'Cagney & Lacey: The View Through the Glass Ceiling',\n",
       "  'Home Improvement',\n",
       "  'Jenny',\n",
       "  'The Pretender',\n",
       "  'The Practice',\n",
       "  'The Nanny',\n",
       "  'Working',\n",
       "  'Any Day Now',\n",
       "  'Two Guys, a Girl and a Pizza Place',\n",
       "  'Bull',\n",
       "  'Ladies Man',\n",
       "  'The Lone Gunmen',\n",
       "  'Becker',\n",
       "  'The West Wing',\n",
       "  'Smallville',\n",
       "  'Crossing Jordan',\n",
       "  'Curb Your Enthusiasm',\n",
       "  'The Division',\n",
       "  'Judging Amy',\n",
       "  'Gilmore Girls',\n",
       "  'Numb3rs',\n",
       "  'Cold Case',\n",
       "  'Celebrity Death Match',\n",
       "  'Bones',\n",
       "  'The King of Queens',\n",
       "  'Private Practice',\n",
       "  'Nip/Tuck',\n",
       "  'Supernatural',\n",
       "  \"Grey's Anatomy\",\n",
       "  'Star Wars: The Clone Wars',\n",
       "  'Archer',\n",
       "  'Wilfred',\n",
       "  'The Legend of Korra',\n",
       "  'Two and a Half Men']}"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the response in the form of html\n",
    "wikiurl=\"https://en.wikipedia.org/wiki/George_Coe#Filmography\"\n",
    "table_class=\"wikitable\"\n",
    "response=requests.get(wikiurl)\n",
    "print(response.status_code)\n",
    "\n",
    "# parse data from the html into a beautifulsoup object\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "indiatable=soup.find_all('table',{'class':\"wikitable\"})[0]\n",
    "\n",
    "df=pd.read_html(str(indiatable))\n",
    "# convert list to dataframe\n",
    "df=pd.DataFrame(df[0])\n",
    "George_Coe_film = []\n",
    "for title in df['Title']:\n",
    "    George_Coe_film.append(title)\n",
    "George_Coe_film\n",
    "\n",
    "# parse data from the html into a beautifulsoup object\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "indiatable=soup.find_all('table',{'class':\"wikitable\"})[1]\n",
    "\n",
    "df=pd.read_html(str(indiatable))\n",
    "# convert list to dataframe\n",
    "df=pd.DataFrame(df[0])\n",
    "George_Coe_tv = []\n",
    "for title in df['Title']:\n",
    "    George_Coe_tv.append(title)\n",
    "George_Coe_tv\n",
    "\n",
    "George_Coe_dict = {'Film':George_Coe_film,'TV':George_Coe_tv}\n",
    "George_Coe_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "b777183c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Film': ['Rabbit Test',\n",
       "  'Likely Stories, Vol. 3',\n",
       "  'This Is Spinal Tap',\n",
       "  'Running Scared',\n",
       "  'The Princess Bride',\n",
       "  'Throw Momma from the Train',\n",
       "  'Memories of Me',\n",
       "  'When Harry Met Sally...',\n",
       "  'City Slickers',\n",
       "  'Horton Hatches the Egg',\n",
       "  'Mr. Saturday Night',\n",
       "  \"City Slickers II: The Legend of Curly's Gold\",\n",
       "  'Forget Paris',\n",
       "  'Hamlet',\n",
       "  \"Fathers' Day\",\n",
       "  'Deconstructing Harry',\n",
       "  'My Giant',\n",
       "  'Analyze This',\n",
       "  'The Adventures of Rocky and Bullwinkle',\n",
       "  '61*',\n",
       "  \"America's Sweethearts\",\n",
       "  'Monsters, Inc.',\n",
       "  \"Mike's New Car\",\n",
       "  'Analyze That',\n",
       "  \"Howl's Moving Castle\",\n",
       "  'Dinotopia: Quest for the Ruby Sunstone',\n",
       "  'Cars',\n",
       "  'Tooth Fairy',\n",
       "  \"I'm Still Here\",\n",
       "  'Small Apartments',\n",
       "  'Parental Guidance',\n",
       "  'Monsters University',\n",
       "  'Party Central',\n",
       "  'The Comedian',\n",
       "  'Untogether',\n",
       "  'Standing Up, Falling Down',\n",
       "  'Here Today'],\n",
       " 'TV': ['All in the Family',\n",
       "  'The Dean Martin Celebrity Roast',\n",
       "  'SST: Death Flight',\n",
       "  'Soap',\n",
       "  'The Love Boat',\n",
       "  'Breaking Up Is Hard to Do',\n",
       "  'Enola Gay: The Men, the Mission, the Atomic Bomb',\n",
       "  'Animalympics',\n",
       "  'Darkroom',\n",
       "  'The Billy Crystal Comedy Hour',\n",
       "  'Saturday Night Live',\n",
       "  'Saturday Night Live',\n",
       "  'Simon & Simon',\n",
       "  'Faerie Tale Theatre',\n",
       "  'Sesame Street',\n",
       "  \"Billy Crystal: Don't Get Me Started\",\n",
       "  \"Billy Crystal: Don't Get Me Started – The Lost Minutes\",\n",
       "  '29th Annual Grammy Awards',\n",
       "  '30th Annual Grammy Awards',\n",
       "  '31st Annual Grammy Awards',\n",
       "  'Billy Crystal: Midnight Train to Moscow',\n",
       "  '62nd Academy Awards',\n",
       "  '63rd Academy Awards',\n",
       "  '64th Academy Awards',\n",
       "  'The Larry Sanders Show',\n",
       "  '65th Academy Awards',\n",
       "  'The Critic',\n",
       "  'Frasier',\n",
       "  'Muppets Tonight',\n",
       "  'Friends',\n",
       "  '69th Academy Awards',\n",
       "  '70th Academy Awards',\n",
       "  '72nd Academy Awards',\n",
       "  'The Bernie Mac Show',\n",
       "  \"Liberty's Kids\",\n",
       "  '76th Academy Awards',\n",
       "  '84th Academy Awards',\n",
       "  'Web Therapy',\n",
       "  '700 Sundays',\n",
       "  'The Comedians',\n",
       "  'Modern Family',\n",
       "  'Monsters at Work']}"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the response in the form of html\n",
    "wikiurl=\"https://en.wikipedia.org/wiki/Billy_Crystal_filmography\"\n",
    "table_class=\"wikitable\"\n",
    "response=requests.get(wikiurl)\n",
    "print(response.status_code)\n",
    "\n",
    "# parse data from the html into a beautifulsoup object\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "indiatable=soup.find_all('table',{'class':\"wikitable\"})[0]\n",
    "\n",
    "df=pd.read_html(str(indiatable))\n",
    "# convert list to dataframe\n",
    "df=pd.DataFrame(df[0])\n",
    "Billy_Crystal_film = []\n",
    "for title in df['Title']:\n",
    "    Billy_Crystal_film.append(title)\n",
    "Billy_Crystal_film\n",
    "\n",
    "# parse data from the html into a beautifulsoup object\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "indiatable=soup.find_all('table',{'class':\"wikitable\"})[1]\n",
    "\n",
    "df=pd.read_html(str(indiatable))\n",
    "# convert list to dataframe\n",
    "df=pd.DataFrame(df[0])\n",
    "Billy_Crystal_tv = []\n",
    "for title in df['Title']:\n",
    "    Billy_Crystal_tv.append(title)\n",
    "Billy_Crystal_tv\n",
    "\n",
    "Billy_Crystal_dict = {'Film':Billy_Crystal_film,'TV':Billy_Crystal_tv}\n",
    "Billy_Crystal_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "0e246850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Film': [\"Mr. Mike's Mondo Video\",\n",
       "  'How to Beat the High Cost of Living',\n",
       "  'O.C. and Stiggs',\n",
       "  'Coneheads',\n",
       "  'Antz',\n",
       "  'Recess: All Growed Down',\n",
       "  \"Geraldine's Fortune\",\n",
       "  'Brooklyn Lobster',\n",
       "  'The Shaggy Dog',\n",
       "  'I Love You, Man',\n",
       "  \"I Don't Know How She Does It\",\n",
       "  'The Heat',\n",
       "  'The Spy Who Dumped Me',\n",
       "  'Can You Ever Forgive Me?',\n",
       "  'Ode to Joy',\n",
       "  'Godmothered',\n",
       "  'Queen Bees'],\n",
       " 'TV': ['Saturday Night Live',\n",
       "  'The Love Boat',\n",
       "  \"What Really Happened to the Class of '65?\",\n",
       "  'Bob & Ray, Jane, Laraine, & Gilda',\n",
       "  'Candida',\n",
       "  'Divorce Wars: A Love Story',\n",
       "  'The Coneheads',\n",
       "  'Bedrooms',\n",
       "  'Kate & Allie',\n",
       "  'American Playhouse',\n",
       "  'Maybe Baby',\n",
       "  'Common Ground',\n",
       "  'Working It Out',\n",
       "  \"Dave's World\",\n",
       "  'Tad',\n",
       "  'Mystery Dance',\n",
       "  '3rd Rock from the Sun',\n",
       "  'Hercules',\n",
       "  'Recess',\n",
       "  'Catch a Falling Star',\n",
       "  'Cyberchase',\n",
       "  'Our Town',\n",
       "  'The Librarian: Quest for the Spear',\n",
       "  'Crumbs',\n",
       "  \"The Librarian: Return to King Solomon's Mines\",\n",
       "  \"Nice Girls Don't Get the Corner Office\",\n",
       "  'In the Motherhood',\n",
       "  'The Librarian: Curse of the Judas Chalice',\n",
       "  'Gary Unmarried',\n",
       "  'Sherri',\n",
       "  'The Women of SNL',\n",
       "  'Rex Is Not Your Lawyer',\n",
       "  'The Oprah Winfrey Show',\n",
       "  'Unforgettable',\n",
       "  'The Librarians',\n",
       "  'The Good Wife',\n",
       "  'Broad City',\n",
       "  'The Good Fight',\n",
       "  'United We Fall']}"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the response in the form of html\n",
    "wikiurl=\"https://en.wikipedia.org/wiki/Jane_Curtin#Filmography\"\n",
    "table_class=\"wikitable\"\n",
    "response=requests.get(wikiurl)\n",
    "print(response.status_code)\n",
    "\n",
    "# parse data from the html into a beautifulsoup object\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "indiatable=soup.find_all('table',{'class':\"wikitable\"})[0]\n",
    "\n",
    "df=pd.read_html(str(indiatable))\n",
    "# convert list to dataframe\n",
    "df=pd.DataFrame(df[0])\n",
    "Jane_Curtin_film = []\n",
    "for title in df['Title']:\n",
    "    Jane_Curtin_film.append(title)\n",
    "Jane_Curtin_film\n",
    "\n",
    "# parse data from the html into a beautifulsoup object\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "indiatable=soup.find_all('table',{'class':\"wikitable\"})[1]\n",
    "\n",
    "df=pd.read_html(str(indiatable))\n",
    "# convert list to dataframe\n",
    "df=pd.DataFrame(df[0])\n",
    "Jane_Curtin_tv = []\n",
    "for title in df['Title']:\n",
    "    Jane_Curtin_tv.append(title)\n",
    "Jane_Curtin_tv\n",
    "\n",
    "Jane_Curtin_dict = {'Film':Jane_Curtin_film,'TV':Jane_Curtin_tv}\n",
    "Jane_Curtin_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "4b85bbb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Film': ['Cutting Loose',\n",
       "  'My Bodyguard',\n",
       "  'Class',\n",
       "  'Sixteen Candles',\n",
       "  'Grandview, U.S.A.',\n",
       "  'The Allnighter',\n",
       "  'Broadcast News',\n",
       "  'Stars and Bars',\n",
       "  'Married to the Mob',\n",
       "  'Working Girl',\n",
       "  'Say Anything...',\n",
       "  \"Men Don't Leave\",\n",
       "  'My Blue Heaven',\n",
       "  'The Cabinet of Dr. Ramirez',\n",
       "  'Hero',\n",
       "  'Toys',\n",
       "  'Addams Family Values',\n",
       "  'Corrina, Corrina',\n",
       "  'Nine Months',\n",
       "  'Two Much',\n",
       "  'Mr. Wrong',\n",
       "  'Grosse Pointe Blank',\n",
       "  'A Smile Like Yours',\n",
       "  'In & Out',\n",
       "  'Arlington Road',\n",
       "  'Cradle Will Rock',\n",
       "  'Runaway Bride',\n",
       "  'Toy Story 2',\n",
       "  'High Fidelity',\n",
       "  'Where the Heart Is',\n",
       "  \"It's a Very Merry Muppet Christmas Movie\",\n",
       "  'School of Rock',\n",
       "  'Looney Tunes: Back in Action',\n",
       "  'Raising Helen',\n",
       "  'The Last Shot',\n",
       "  'Ice Princess',\n",
       "  'Chicken Little',\n",
       "  'Friends with Money',\n",
       "  'Martian Child',\n",
       "  'War, Inc.',\n",
       "  'Kit Kittredge: An American Girl',\n",
       "  'Confessions of a Shopaholic',\n",
       "  \"My Sister's Keeper\",\n",
       "  'Acceptance',\n",
       "  'Toy Story 3',\n",
       "  'Hoodwinked Too! Hood vs. Evil',\n",
       "  'Mars Needs Moms',\n",
       "  'Hawaiian Vacation',\n",
       "  'Small Fry',\n",
       "  'Arthur Christmas',\n",
       "  'Partysaurus Rex',\n",
       "  'The Perks of Being a Wallflower',\n",
       "  'Welcome to Me',\n",
       "  'The End of the Tour',\n",
       "  'Freaks of Nature',\n",
       "  'Popstar: Never Stop Never Stopping',\n",
       "  'Snatched',\n",
       "  'Unicorn Store',\n",
       "  'Instant Family',\n",
       "  'Toy Story 4',\n",
       "  'Let It Snow',\n",
       "  'Klaus'],\n",
       " 'TV': ['Saturday Night Live',\n",
       "  'What About Joan?',\n",
       "  'Peep and the Big Wide World',\n",
       "  'Law & Order: Special Victims Unit',\n",
       "  'Shameless',\n",
       "  'Phineas and Ferb',\n",
       "  'The Office',\n",
       "  'Toy Story of Terror!',\n",
       "  'Toy Story That Time Forgot',\n",
       "  'The Stinky & Dirty Show',\n",
       "  'A Series of Unfortunate Events',\n",
       "  'The Christmas Train',\n",
       "  'Homecoming']}"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the response in the form of html\n",
    "wikiurl=\"https://en.wikipedia.org/wiki/Joan_Cusack#Filmography\"\n",
    "table_class=\"wikitable\"\n",
    "response=requests.get(wikiurl)\n",
    "print(response.status_code)\n",
    "\n",
    "# parse data from the html into a beautifulsoup object\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "indiatable=soup.find_all('table',{'class':\"wikitable\"})[0]\n",
    "\n",
    "df=pd.read_html(str(indiatable))\n",
    "# convert list to dataframe\n",
    "df=pd.DataFrame(df[0])\n",
    "Joan_Cusack_film = []\n",
    "for title in df['Title']:\n",
    "    Joan_Cusack_film.append(title)\n",
    "Joan_Cusack_film\n",
    "\n",
    "# parse data from the html into a beautifulsoup object\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "indiatable=soup.find_all('table',{'class':\"wikitable\"})[1]\n",
    "\n",
    "df=pd.read_html(str(indiatable))\n",
    "# convert list to dataframe\n",
    "df=pd.DataFrame(df[0])\n",
    "Joan_Cusack_tv = []\n",
    "for title in df['Title']:\n",
    "    Joan_Cusack_tv.append(title)\n",
    "Joan_Cusack_tv\n",
    "\n",
    "Joan_Cusack_dict = {'Film':Joan_Cusack_film,'TV':Joan_Cusack_tv}\n",
    "Joan_Cusack_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "fd961a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Film': ['School Dance',\n",
       "  'Trainwreck',\n",
       "  'Set It Up',\n",
       "  'What Men Want',\n",
       "  'Big Time Adolescence',\n",
       "  'The Dirt',\n",
       "  'The Angry Birds Movie 2',\n",
       "  'The Jesus Rolls',\n",
       "  'The King of Staten Island',\n",
       "  'The Suicide Squad',\n",
       "  'I Want You Back',\n",
       "  'Bodies, Bodies, Bodies',\n",
       "  'Good Mourning[95]',\n",
       "  'Meet Cute',\n",
       "  'The Home'],\n",
       " 'TV': [\"Wild 'N Out\",\n",
       "  'Brooklyn Nine-Nine',\n",
       "  'Friends of the People',\n",
       "  'Saturday Night Live',\n",
       "  'The Jim Gaffigan Show',\n",
       "  'Pete Davidson: SMD',\n",
       "  'Eighty-Sixed',\n",
       "  'Click, Clack, Moo: Christmas at the Farm',\n",
       "  'The Guest Book',\n",
       "  'Pete Davidson: Alive from New York',\n",
       "  'The Real Bros of Simi Valley',\n",
       "  'The Rookie',\n",
       "  'The Freak Brothers',\n",
       "  \"Ricky Velez: Here's Everything\",\n",
       "  'The Now',\n",
       "  'The Kids in the Hall']}"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the response in the form of html\n",
    "wikiurl=\"https://en.wikipedia.org/wiki/Pete_Davidson#Filmography\"\n",
    "table_class=\"wikitable\"\n",
    "response=requests.get(wikiurl)\n",
    "print(response.status_code)\n",
    "\n",
    "# parse data from the html into a beautifulsoup object\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "indiatable=soup.find_all('table',{'class':\"wikitable\"})[0]\n",
    "\n",
    "df=pd.read_html(str(indiatable))\n",
    "# convert list to dataframe\n",
    "df=pd.DataFrame(df[0])\n",
    "Pete_Davidson_film = []\n",
    "for title in df['Title']:\n",
    "    Pete_Davidson_film.append(title)\n",
    "Pete_Davidson_film\n",
    "\n",
    "# parse data from the html into a beautifulsoup object\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "indiatable=soup.find_all('table',{'class':\"wikitable\"})[1]\n",
    "\n",
    "df=pd.read_html(str(indiatable))\n",
    "# convert list to dataframe\n",
    "df=pd.DataFrame(df[0])\n",
    "Pete_Davidson_tv = []\n",
    "for title in df['Title']:\n",
    "    Pete_Davidson_tv.append(title)\n",
    "Pete_Davidson_tv\n",
    "\n",
    "Pete_Davidson_dict = {'Film':Pete_Davidson_film,'TV':Pete_Davidson_tv}\n",
    "Pete_Davidson_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "4c2fae33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Film': ['Saturday Night Live',\n",
       "  'Tunnel Vision',\n",
       "  'The Paul Simon Special',\n",
       "  'Bob & Ray, Jane, Laraine & Gilda',\n",
       "  \"Steve Martin's Best Show Ever\",\n",
       "  'Trading Places',\n",
       "  'The Coneheads',\n",
       "  'Franken and Davis at Stockton State',\n",
       "  'The New Show',\n",
       "  'One More Saturday Night',\n",
       "  'The Feud',\n",
       "  'Coneheads',\n",
       "  'Trailer Park',\n",
       "  'Blues Brothers 2000',\n",
       "  'Evolution'],\n",
       " 'TV': ['Saturday Night Live',\n",
       "  'Tunnel Vision',\n",
       "  'The Paul Simon Special',\n",
       "  'Bob & Ray, Jane, Laraine & Gilda',\n",
       "  \"Steve Martin's Best Show Ever\",\n",
       "  'Trading Places',\n",
       "  'The Coneheads',\n",
       "  'Franken and Davis at Stockton State',\n",
       "  'The New Show',\n",
       "  'One More Saturday Night',\n",
       "  'The Feud',\n",
       "  'Coneheads',\n",
       "  'Trailer Park',\n",
       "  'Blues Brothers 2000',\n",
       "  'Evolution']}"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# FIX THIS ONE!!\n",
    "\n",
    "# get the response in the form of html\n",
    "wikiurl=\"https://en.wikipedia.org/wiki/Tom_Davis_(comedian)#Filmography\"\n",
    "table_class=\"wikitable\"\n",
    "response=requests.get(wikiurl)\n",
    "print(response.status_code)\n",
    "\n",
    "# parse data from the html into a beautifulsoup object\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "indiatable=soup.find_all('table',{'class':\"wikitable\"})[0]\n",
    "\n",
    "df=pd.read_html(str(indiatable))\n",
    "# convert list to dataframe\n",
    "df=pd.DataFrame(df[0])\n",
    "Tom_Davis_film = []\n",
    "for title in df['Title']:\n",
    "    Tom_Davis_film.append(title)\n",
    "Tom_Davis_film\n",
    "\n",
    "# # parse data from the html into a beautifulsoup object\n",
    "# soup = BeautifulSoup(response.text, 'html.parser')\n",
    "# indiatable=soup.find_all('table',{'class':\"wikitable\"})[1]\n",
    "\n",
    "df=pd.read_html(str(indiatable))\n",
    "# convert list to dataframe\n",
    "df=pd.DataFrame(df[0])\n",
    "Tom_Davis_tv = []\n",
    "for title in df['Title']:\n",
    "    Tom_Davis_tv.append(title)\n",
    "Tom_Davis_tv\n",
    "\n",
    "Tom_Davis_dict = {'Film':Tom_Davis_film,'TV':Tom_Davis_tv}\n",
    "Tom_Davis_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "486627cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Film': ['Brother Nature',\n",
       "  'Brittany Runs a Marathon',\n",
       "  'Little',\n",
       "  'Hubie Halloween[44]',\n",
       "  'Home Sweet Home Alone'],\n",
       " 'TV': ['The Mike & Ben Show',\n",
       "  'Angel',\n",
       "  'Faking the Video',\n",
       "  'World Cup Comedy',\n",
       "  \"Wild 'n Out\",\n",
       "  'Reno 911!',\n",
       "  'Totally Awesome',\n",
       "  'The Underground',\n",
       "  'Short Circuitz',\n",
       "  'Say What? Karaoke 2.0',\n",
       "  'Online Nation',\n",
       "  'Bar Starz',\n",
       "  'Kath & Kim',\n",
       "  'Nathan vs. Nurture',\n",
       "  'Mad',\n",
       "  'Friends with Benefits',\n",
       "  'Animation Domination High-Def',\n",
       "  'Incredible Crew',\n",
       "  'Robot Chicken',\n",
       "  'Idiotsitter',\n",
       "  'Maya & Marty',\n",
       "  'Saturday Night Live',\n",
       "  'The David S. Pumpkins Halloween Special',\n",
       "  'Is It Cake?']}"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the response in the form of html\n",
    "wikiurl=\"https://en.wikipedia.org/wiki/Mikey_Day#Filmography\"\n",
    "table_class=\"wikitable\"\n",
    "response=requests.get(wikiurl)\n",
    "print(response.status_code)\n",
    "\n",
    "# parse data from the html into a beautifulsoup object\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "indiatable=soup.find_all('table',{'class':\"wikitable\"})[0]\n",
    "\n",
    "df=pd.read_html(str(indiatable))\n",
    "# convert list to dataframe\n",
    "df=pd.DataFrame(df[0])\n",
    "Mikey_Day_film = []\n",
    "for title in df['Title']:\n",
    "    Mikey_Day_film.append(title)\n",
    "Mikey_Day_film\n",
    "\n",
    "# parse data from the html into a beautifulsoup object\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "indiatable=soup.find_all('table',{'class':\"wikitable\"})[1]\n",
    "\n",
    "df=pd.read_html(str(indiatable))\n",
    "# convert list to dataframe\n",
    "df=pd.DataFrame(df[0])\n",
    "Mikey_Day_tv = []\n",
    "for title in df['Title']:\n",
    "    Mikey_Day_tv.append(title)\n",
    "Mikey_Day_tv\n",
    "\n",
    "Mikey_Day_dict = {'Film':Mikey_Day_film,'TV':Mikey_Day_tv}\n",
    "Mikey_Day_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "d3a09c7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Film': ['The Brain Machine',\n",
       "  'Bum Rap',\n",
       "  \"Wayne's World 2\",\n",
       "  'Billy Madison',\n",
       "  'The Little Patriot',\n",
       "  'Dirty Work',\n",
       "  'There Will Be Blood'],\n",
       " 'TV': ['Saturday Night Live',\n",
       "  \"Steve Martin's Best Show Ever\",\n",
       "  'Late Night with David Letterman',\n",
       "  'The New Show',\n",
       "  'Kate & Allie',\n",
       "  'Saturday Night Live Weekend Update Thursday',\n",
       "  'Curb Your Enthusiasm',\n",
       "  '30 Rock']}"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the response in the form of html\n",
    "wikiurl=\"https://en.wikipedia.org/wiki/Jim_Downey_(comedian)#Filmography\"\n",
    "table_class=\"wikitable\"\n",
    "response=requests.get(wikiurl)\n",
    "print(response.status_code)\n",
    "\n",
    "# parse data from the html into a beautifulsoup object\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "indiatable=soup.find_all('table',{'class':\"wikitable\"})[1]\n",
    "\n",
    "df=pd.read_html(str(indiatable))\n",
    "# convert list to dataframe\n",
    "df=pd.DataFrame(df[0])\n",
    "Jim_Downey_film = []\n",
    "for title in df['Title']:\n",
    "    Jim_Downey_film.append(title)\n",
    "Jim_Downey_film\n",
    "\n",
    "# parse data from the html into a beautifulsoup object\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "indiatable=soup.find_all('table',{'class':\"wikitable\"})[0]\n",
    "\n",
    "df=pd.read_html(str(indiatable))\n",
    "# convert list to dataframe\n",
    "df=pd.DataFrame(df[0])\n",
    "Jim_Downey_tv = []\n",
    "for title in df['Title']:\n",
    "    Jim_Downey_tv.append(title)\n",
    "Jim_Downey_tv\n",
    "\n",
    "Jim_Downey_dict = {'Film':Jim_Downey_film,'TV':Jim_Downey_tv}\n",
    "Jim_Downey_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "7cb9d7f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Film': ['Pound',\n",
       "  \"Greaser's Palace\",\n",
       "  'Two Tons of Turquoise to Taos Tonight',\n",
       "  'Up the Academy',\n",
       "  \"Baby It's You\",\n",
       "  'Firstborn',\n",
       "  'Tuff Turf',\n",
       "  'Girls Just Want to Have Fun',\n",
       "  'Weird Science',\n",
       "  'Deadwait',\n",
       "  'Back to School',\n",
       "  'The Pick-up Artist',\n",
       "  'Less Than Zero',\n",
       "  'Johnny Be Good',\n",
       "  'Rented Lips',\n",
       "  '1969',\n",
       "  \"That's Adequate\",\n",
       "  'True Believer',\n",
       "  'Chances Are',\n",
       "  'Air America',\n",
       "  'Too Much Sun',\n",
       "  'Soapdish',\n",
       "  'Chaplin',\n",
       "  'Heart and Souls',\n",
       "  'The Last Party',\n",
       "  'Short Cuts',\n",
       "  'Hail Caesar',\n",
       "  'Natural Born Killers',\n",
       "  'Only You',\n",
       "  'Richard III',\n",
       "  'Home for the Holidays',\n",
       "  'Restoration',\n",
       "  'Danger Zone',\n",
       "  'One Night Stand',\n",
       "  'Two Girls and a Guy',\n",
       "  'Hugo Pool',\n",
       "  'The Gingerbread Man',\n",
       "  'U.S. Marshals',\n",
       "  'In Dreams',\n",
       "  'Friends & Lovers',\n",
       "  'Bowfinger',\n",
       "  'Black and White',\n",
       "  'Wonder Boys',\n",
       "  'Lethargy',\n",
       "  'Whatever We Do',\n",
       "  'The Singing Detective',\n",
       "  'Charlie: The Life and Art of Charles Chaplin',\n",
       "  'Gothika',\n",
       "  'Eros',\n",
       "  'Game 6',\n",
       "  'Kiss Kiss Bang Bang',\n",
       "  'The Outsider',\n",
       "  'Good Night, and Good Luck',\n",
       "  'Hubert Selby Jr.: It/ll Be Better Tomorrow',\n",
       "  'A Guide to Recognizing Your Saints',\n",
       "  'The Shaggy Dog',\n",
       "  'A Scanner Darkly',\n",
       "  'Fur',\n",
       "  'Zodiac',\n",
       "  'Lucky You',\n",
       "  'Charlie Bartlett',\n",
       "  'Iron Man',\n",
       "  'The Incredible Hulk',\n",
       "  'Tropic Thunder',\n",
       "  'The Soloist',\n",
       "  'Sherlock Holmes',\n",
       "  'Iron Man 2',\n",
       "  'Love & Distrust',\n",
       "  'Due Date',\n",
       "  'Sherlock Holmes: A Game of Shadows',\n",
       "  'The Avengers',\n",
       "  'Iron Man 3',\n",
       "  'The Judge',\n",
       "  'Chef',\n",
       "  'Avengers: Age of Ultron',\n",
       "  'The Nice Guys',\n",
       "  'Captain America: Civil War',\n",
       "  'Spider-Man: Homecoming',\n",
       "  'Avengers: Infinity War',\n",
       "  'Avengers: Endgame',\n",
       "  'Dolittle',\n",
       "  'Oppenheimer',\n",
       "  'All-Star Weekend'],\n",
       " 'TV': ['Saturday Night Live',\n",
       "  'Mussolini: The Untold Story',\n",
       "  \"Mr. Willowby's Christmas Tree\",\n",
       "  'Saturday Night Live',\n",
       "  'Ally McBeal',\n",
       "  'Family Guy',\n",
       "  'The Age of A.I.',\n",
       "  'Perry Mason',\n",
       "  'Sweet Tooth',\n",
       "  'The Sympathizer']}"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the response in the form of html\n",
    "wikiurl=\"https://en.wikipedia.org/wiki/Robert_Downey_Jr._filmography\"\n",
    "table_class=\"wikitable\"\n",
    "response=requests.get(wikiurl)\n",
    "print(response.status_code)\n",
    "\n",
    "# parse data from the html into a beautifulsoup object\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "indiatable=soup.find_all('table',{'class':\"wikitable\"})[0]\n",
    "\n",
    "df=pd.read_html(str(indiatable))\n",
    "# convert list to dataframe\n",
    "df=pd.DataFrame(df[0])\n",
    "Robert_Downey_Jr_film = []\n",
    "for title in df['Title']:\n",
    "    Robert_Downey_Jr_film.append(title)\n",
    "Robert_Downey_Jr_film\n",
    "\n",
    "# parse data from the html into a beautifulsoup object\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "indiatable=soup.find_all('table',{'class':\"wikitable\"})[1]\n",
    "\n",
    "df=pd.read_html(str(indiatable))\n",
    "# convert list to dataframe\n",
    "df=pd.DataFrame(df[0])\n",
    "Robert_Downey_Jr_tv = []\n",
    "for title in df['Title']:\n",
    "    Robert_Downey_Jr_tv.append(title)\n",
    "Robert_Downey_Jr_tv\n",
    "\n",
    "Robert_Downey_Jr_dict = {'Film':Robert_Downey_Jr_film,'TV':Robert_Downey_Jr_tv}\n",
    "Robert_Downey_Jr_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "ac98da64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Film': ['Fuzz',\n",
       "  'Tarzoon: Shame of the Jungle',\n",
       "  'Caddyshack',\n",
       "  'Modern Problems',\n",
       "  \"National Lampoon's Vacation\",\n",
       "  'Sixteen Candles',\n",
       "  \"The Razor's Edge\",\n",
       "  'Head Office',\n",
       "  'Legal Eagles',\n",
       "  'Club Paradise',\n",
       "  'Scrooged',\n",
       "  'Superman 50th Anniversary',\n",
       "  'The Experts',\n",
       "  'How I Got into College',\n",
       "  'Ghostbusters II',\n",
       "  \"National Lampoon's Christmas Vacation\",\n",
       "  'Small White House',\n",
       "  'Nothing but Trouble',\n",
       "  'Babe Ruth',\n",
       "  'JFK',\n",
       "  \"Wayne's World\",\n",
       "  'Groundhog Day',\n",
       "  'Cabin Boy',\n",
       "  \"My Brother's Keeper\",\n",
       "  'Jury Duty',\n",
       "  'Multiplicity',\n",
       "  'Duckman',\n",
       "  'Waiting for Guffman',\n",
       "  'The Brave Little Toaster to the Rescue',\n",
       "  'As Good as It Gets',\n",
       "  'The Brave Little Toaster Goes to Mars',\n",
       "  'Dennis the Menace Strikes Again',\n",
       "  'Dr. Dolittle',\n",
       "  \"Jungle Book: Mowgli's Story\",\n",
       "  'Bedazzled',\n",
       "  'Snow Dogs',\n",
       "  \"A Gentleman's Game\",\n",
       "  'Getting Hal',\n",
       "  'Daddy Day Camp',\n",
       "  'Love Comes Lately',\n",
       "  '17 Again',\n",
       "  'Eye of the Hurricane',\n",
       "  'The Three Stooges',\n",
       "  'Underdogs',\n",
       "  'For the Fun of the Game'],\n",
       " 'TV': ['Saturday Night Live with Howard Cosell',\n",
       "  'The TVTV Show',\n",
       "  'Saturday Night Live',\n",
       "  'Good Sports',\n",
       "  'Get a Life',\n",
       "  'Married... with Children',\n",
       "  'Wings',\n",
       "  'Frosty Returns',\n",
       "  'Seinfeld',\n",
       "  'Bakersfield P.D.',\n",
       "  'Lois & Clark: The New Adventures of Superman',\n",
       "  'Ellen',\n",
       "  'Nightmare Ned',\n",
       "  'Casper: A Spirited Beginning',\n",
       "  'Aaahh!!! Real Monsters',\n",
       "  'Between Brothers',\n",
       "  'Mr. Show with Bob and David',\n",
       "  'Smart Guy',\n",
       "  'Stuart Little',\n",
       "  'Love & Money',\n",
       "  'SpongeBob SquarePants',\n",
       "  'Jackie Chan Adventures',\n",
       "  'Buzz Lightyear of Star Command',\n",
       "  'King of the Hill',\n",
       "  'Family Guy',\n",
       "  'The Angry Beavers',\n",
       "  'Yes, Dear',\n",
       "  'Teamo Supremo',\n",
       "  'Justice League',\n",
       "  'The Buzz on Maggie',\n",
       "  \"My Gym Partner's a Monkey\",\n",
       "  'Tom Goes to the Mayor',\n",
       "  'The Marvelous Misadventures of Flapjack',\n",
       "  'The Goode Family',\n",
       "  'The Middle',\n",
       "  'WordGirl',\n",
       "  'Kick Buttowski: Suburban Daredevil',\n",
       "  'Supernatural',\n",
       "  'Adventure Time',\n",
       "  'Motorcity',\n",
       "  'Sullivan & Son',\n",
       "  'Raising Hope',\n",
       "  'Fish Hooks',\n",
       "  '2 Broke Girls',\n",
       "  'Christmas Under Wraps',\n",
       "  \"It's Always Sunny in Philadelphia\",\n",
       "  'Veep',\n",
       "  'The Daily Show with Trevor Noah',\n",
       "  \"Billy Dilley's Super-Duper Subterranean Summer\",\n",
       "  \"Bill Murray & Brian Doyle-Murray's Extra Innings\",\n",
       "  'Lodge 49',\n",
       "  \"Kamp Koral: SpongeBob's Under Years\",\n",
       "  'The Patrick Star Show']}"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the response in the form of html\n",
    "wikiurl=\"https://en.wikipedia.org/wiki/Brian_Doyle_Murray#Filmography\"\n",
    "table_class=\"wikitable\"\n",
    "response=requests.get(wikiurl)\n",
    "print(response.status_code)\n",
    "\n",
    "# parse data from the html into a beautifulsoup object\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "indiatable=soup.find_all('table',{'class':\"wikitable\"})[0]\n",
    "\n",
    "df=pd.read_html(str(indiatable))\n",
    "# convert list to dataframe\n",
    "df=pd.DataFrame(df[0])\n",
    "Brian_Doyle_Murray_film = []\n",
    "for title in df['Title']:\n",
    "    Brian_Doyle_Murray_film.append(title)\n",
    "Brian_Doyle_Murray_film\n",
    "\n",
    "# parse data from the html into a beautifulsoup object\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "indiatable=soup.find_all('table',{'class':\"wikitable\"})[1]\n",
    "\n",
    "df=pd.read_html(str(indiatable))\n",
    "# convert list to dataframe\n",
    "df=pd.DataFrame(df[0])\n",
    "Brian_Doyle_Murray_tv = []\n",
    "for title in df['Title']:\n",
    "    Brian_Doyle_Murray_tv.append(title)\n",
    "Brian_Doyle_Murray_tv\n",
    "\n",
    "Brian_Doyle_Murray_dict = {'Film':Brian_Doyle_Murray_film,'TV':Brian_Doyle_Murray_tv}\n",
    "Brian_Doyle_Murray_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "1c479bf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Film': ['Serious Business',\n",
       "  'Martin & Orloff',\n",
       "  'The Hebrew Hammer',\n",
       "  'Down With Love',\n",
       "  \"National Lampoon's Barely Legal\",\n",
       "  'Dickie Roberts: Former Child Star',\n",
       "  'Freshman Orientation',\n",
       "  'Looking for Kitty',\n",
       "  'Her Minor Thing',\n",
       "  'Winter Passing',\n",
       "  'Click',\n",
       "  'The Pleasure Drivers',\n",
       "  'I Now Pronounce You Chuck & Larry',\n",
       "  'Bill',\n",
       "  'Harold',\n",
       "  'Spring Breakdown',\n",
       "  \"Love N' Dancing\",\n",
       "  \"I Hate Valentine's Day\",\n",
       "  'My Life in Ruins',\n",
       "  'Just Go with It',\n",
       "  'Teacher of The Year',\n",
       "  \"That's My Boy\",\n",
       "  'Syrup',\n",
       "  'A Little Game',\n",
       "  'The Grief of Others',\n",
       "  'Sisters',\n",
       "  'Hurricane Bianca',\n",
       "  'Tracktown',\n",
       "  'The Week Of',\n",
       "  'Hurricane Bianca 2: From Russia with Hate',\n",
       "  'Little',\n",
       "  'Wine Country',\n",
       "  'Plan B',\n",
       "  'A Clüsterfünke Christmas',\n",
       "  'I Love My Dad'],\n",
       " 'TV': ['Saturday Night Live',\n",
       "  'Third Watch',\n",
       "  'Kim Possible',\n",
       "  'The King of Queens',\n",
       "  'Soundtracks Live',\n",
       "  'Monk',\n",
       "  'Game Over',\n",
       "  'Frasier',\n",
       "  \"O'Grady\",\n",
       "  '30 Rock',\n",
       "  'Aqua Teen Hunger Force',\n",
       "  'Squidbillies',\n",
       "  'Assy McGee',\n",
       "  'Avatar: The Last Airbender',\n",
       "  'The Consultants',\n",
       "  'Superjail!',\n",
       "  'Wizards of Waverly Place',\n",
       "  'Ugly Betty',\n",
       "  'Sherri',\n",
       "  'Delocated',\n",
       "  'Fish Hooks',\n",
       "  'Funny or Die Presents',\n",
       "  'Lady Friends',\n",
       "  'Up All Night',\n",
       "  'Suburgatory',\n",
       "  'The Cleveland Show',\n",
       "  'The Middle',\n",
       "  'The Awesomes',\n",
       "  'Broad City',\n",
       "  'The Neighbors',\n",
       "  'Inside Amy Schumer',\n",
       "  \"Bob's Burgers\",\n",
       "  'Dead Boss',\n",
       "  'Parks and Recreation',\n",
       "  'Salem Rogers: Model of the Year 1998',\n",
       "  'Last Week Tonight with John Oliver',\n",
       "  'Unforgettable',\n",
       "  'Difficult People',\n",
       "  'The $100,000 Pyramid',\n",
       "  'The Simpsons',\n",
       "  \"Rachel Dratch's Late Night Snack\",\n",
       "  'Imaginary Mary',\n",
       "  'Angie Tribeca',\n",
       "  'Great News',\n",
       "  'Unbreakable Kimmy Schmidt',\n",
       "  'At Home with Amy Sedaris',\n",
       "  'Portlandia',\n",
       "  'Teen Titans Go!',\n",
       "  'Shameless',\n",
       "  \"Blue's Clues & You!\",\n",
       "  'Harley Quinn',\n",
       "  'The Good Fight',\n",
       "  'Mr. Mayor',\n",
       "  'Bubble Guppies',\n",
       "  'American Dad',\n",
       "  'Getting Curious with Jonathan Van Ness']}"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the response in the form of html\n",
    "wikiurl=\"https://en.wikipedia.org/wiki/Rachel_Dratch#Filmography\"\n",
    "table_class=\"wikitable\"\n",
    "response=requests.get(wikiurl)\n",
    "print(response.status_code)\n",
    "\n",
    "# parse data from the html into a beautifulsoup object\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "indiatable=soup.find_all('table',{'class':\"wikitable\"})[1]\n",
    "\n",
    "df=pd.read_html(str(indiatable))\n",
    "# convert list to dataframe\n",
    "df=pd.DataFrame(df[0])\n",
    "Rachel_Dratch_film = []\n",
    "for title in df['Title']:\n",
    "    Rachel_Dratch_film.append(title)\n",
    "Rachel_Dratch_film\n",
    "\n",
    "# parse data from the html into a beautifulsoup object\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "indiatable=soup.find_all('table',{'class':\"wikitable\"})[0]\n",
    "\n",
    "df=pd.read_html(str(indiatable))\n",
    "# convert list to dataframe\n",
    "df=pd.DataFrame(df[0])\n",
    "Rachel_Dratch_tv = []\n",
    "for title in df['Title']:\n",
    "    Rachel_Dratch_tv.append(title)\n",
    "Rachel_Dratch_tv\n",
    "\n",
    "Rachel_Dratch_dict = {'Film':Rachel_Dratch_film,'TV':Rachel_Dratch_tv}\n",
    "Rachel_Dratch_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "59872a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Film': ['Running',\n",
       "  'Club Paradise',\n",
       "  'Blue Monkey',\n",
       "  'Motorama',\n",
       "  'Only You',\n",
       "  'There Goes the Neighborhood',\n",
       "  'Groundhog Day',\n",
       "  'I Love Trouble',\n",
       "  'Stuart Saves His Family',\n",
       "  'Multiplicity',\n",
       "  'Portrait of a Serial Monogamist'],\n",
       " 'TV': ['Second City Television',\n",
       "  'King of Kensington',\n",
       "  'Saturday Night Live',\n",
       "  'The Last Polka',\n",
       "  'The Second City Toronto 15th Anniversary',\n",
       "  'I, Martin Short, Goes Hollywood',\n",
       "  'Maniac Mansion',\n",
       "  'Hostage for a Day',\n",
       "  'The Adventures of Dudley the Dragon',\n",
       "  'Boston Common',\n",
       "  'North of 60',\n",
       "  \"Stickin' Around\",\n",
       "  'F/X: The Series',\n",
       "  'The Adventures of Sam & Max: Freelance Police',\n",
       "  'The Outer Limits',\n",
       "  'Degas and the Dancer',\n",
       "  'Bad Dog',\n",
       "  'George and Martha',\n",
       "  \"Blaster's Universe\",\n",
       "  'Quints',\n",
       "  'The Sandy Bottom Orchestra',\n",
       "  'Marvin the Tap-Dancing Horse',\n",
       "  'Virtual Mom',\n",
       "  'Bob and Margaret',\n",
       "  'Monk',\n",
       "  'Atomic Betty',\n",
       "  'Train 48',\n",
       "  '6teen',\n",
       "  'Getting Along Famously',\n",
       "  'The Jane Show',\n",
       "  'I, Martin Short, Goes Home',\n",
       "  'Comedy Bar',\n",
       "  'Rocky Road',\n",
       "  'Man Seeking Woman',\n",
       "  \"Schitt's Creek\",\n",
       "  'Max & Ruby',\n",
       "  'Bigfoot']}"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the response in the form of html\n",
    "wikiurl=\"https://en.wikipedia.org/wiki/Robin_Duke#Filmography\"\n",
    "table_class=\"wikitable\"\n",
    "response=requests.get(wikiurl)\n",
    "print(response.status_code)\n",
    "\n",
    "# parse data from the html into a beautifulsoup object\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "indiatable=soup.find_all('table',{'class':\"wikitable\"})[0]\n",
    "\n",
    "df=pd.read_html(str(indiatable))\n",
    "# convert list to dataframe\n",
    "df=pd.DataFrame(df[0])\n",
    "Robin_Duke_film = []\n",
    "for title in df['Title']:\n",
    "    Robin_Duke_film.append(title)\n",
    "Robin_Duke_film\n",
    "\n",
    "# parse data from the html into a beautifulsoup object\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "indiatable=soup.find_all('table',{'class':\"wikitable\"})[1]\n",
    "\n",
    "df=pd.read_html(str(indiatable))\n",
    "# convert list to dataframe\n",
    "df=pd.DataFrame(df[0])\n",
    "Robin_Duke_tv = []\n",
    "for title in df['Title']:\n",
    "    Robin_Duke_tv.append(title)\n",
    "Robin_Duke_tv\n",
    "\n",
    "Robin_Duke_dict = {'Film':Robin_Duke_film,'TV':Robin_Duke_tv}\n",
    "Robin_Duke_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "23153186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Film': ['Working Girl',\n",
       "  'How I Got Into College',\n",
       "  'Miami Blues',\n",
       "  'Stepping Out',\n",
       "  'Passion Fish',\n",
       "  'Born Yesterday',\n",
       "  'I Love Trouble',\n",
       "  'The Last Supper',\n",
       "  'Bulworth',\n",
       "  'Air Bud: Golden Receiver',\n",
       "  'The Thin Pink Line',\n",
       "  'Drop Dead Gorgeous',\n",
       "  'Three Kings',\n",
       "  'What Planet Are You From?',\n",
       "  'Heartbreakers',\n",
       "  \"What's the Worst That Could Happen?\",\n",
       "  'Zoolander',\n",
       "  \"Max Keeble's Big Move\",\n",
       "  'Cherish',\n",
       "  'Storm Watch',\n",
       "  'Die, Mommie, Die!',\n",
       "  'The Hebrew Hammer',\n",
       "  'Bruce Almighty',\n",
       "  'Out of Time',\n",
       "  'Runaway Jury',\n",
       "  'November',\n",
       "  'Paper Cut',\n",
       "  'Laws of Attraction',\n",
       "  'The Civilization of Maxwell Bright',\n",
       "  'Love for Rent',\n",
       "  'The Prize Winner of Defiance, Ohio',\n",
       "  'The Darwin Awards',\n",
       "  'Pucked',\n",
       "  'Southland Tales',\n",
       "  'Pineapple Express',\n",
       "  'The Answer Man',\n",
       "  \"Archie's Final Project\",\n",
       "  \"It's Complicated\",\n",
       "  'LOL',\n",
       "  '3, 2, 1... Frankie Go Boom',\n",
       "  'The Guilt Trip',\n",
       "  'Entourage',\n",
       "  'A Light Beneath Their Feet',\n",
       "  'Dude',\n",
       "  'Canal Street',\n",
       "  'Tag',\n",
       "  'The Oath',\n",
       "  'Boy Genius',\n",
       "  'The Lost Husband',\n",
       "  'Together Together',\n",
       "  'The Hater'],\n",
       " 'TV': ['Saturday Night Live',\n",
       "  'Get a Life',\n",
       "  'Civil Wars',\n",
       "  \"Basic Values: Sex, Shock & Censorship in the 90's\",\n",
       "  'Sisters',\n",
       "  'Shake, Rattle and Rock!',\n",
       "  'The Nanny',\n",
       "  'I Am Weasel',\n",
       "  'Cybill',\n",
       "  'Extreme Ghostbusters',\n",
       "  'Everybody Loves Raymond',\n",
       "  'George and Leo',\n",
       "  'Pinky and the Brain',\n",
       "  'The X-Files',\n",
       "  'The Nanny',\n",
       "  'Histeria!',\n",
       "  'The Wild Thornberrys',\n",
       "  'Chicken Soup for the Soul',\n",
       "  'Futurama',\n",
       "  'The Chronicle',\n",
       "  'Weakest Link',\n",
       "  'Curb Your Enthusiasm',\n",
       "  'The Outer Limits',\n",
       "  'Knee High P.I.',\n",
       "  'Just Shoot Me!',\n",
       "  'CSI: Miami',\n",
       "  'Wild Card',\n",
       "  'See Arnold Run',\n",
       "  'LAX',\n",
       "  'Amber Frey: Witness for the Prosecution',\n",
       "  'Stacked',\n",
       "  'Law & Order',\n",
       "  'Three Moons Over Milford',\n",
       "  'Shark',\n",
       "  'Entourage',\n",
       "  'Boston Legal',\n",
       "  'Aliens in America',\n",
       "  'Numbers',\n",
       "  'Criminal Minds',\n",
       "  'Pushing Daisies',\n",
       "  'Samantha Who?',\n",
       "  \"It's Always Sunny in Philadelphia\",\n",
       "  'As Told by Ginger',\n",
       "  'Private Practice',\n",
       "  'Psych',\n",
       "  'The Defenders',\n",
       "  \"Harry's Law\",\n",
       "  \"Don't Trust the B---- in Apartment 23\",\n",
       "  'The Neighbors',\n",
       "  'Franklin & Bash',\n",
       "  'Bones',\n",
       "  'Hot in Cleveland',\n",
       "  'Mulaney',\n",
       "  'Sirens',\n",
       "  \"Girlfriends' Guide to Divorce\",\n",
       "  'Best Friends Whenever',\n",
       "  'New Girl',\n",
       "  'Life in Pieces',\n",
       "  '*Loosely Exactly Nicole',\n",
       "  'Grace and Frankie',\n",
       "  'Chicago Med',\n",
       "  'Graves',\n",
       "  '2 Broke Girls',\n",
       "  'Chicago P.D.',\n",
       "  'Worst Cooks in America',\n",
       "  'How to Get Away with Murder',\n",
       "  'The Librarians',\n",
       "  'Detroiters',\n",
       "  'The Boss Baby: Back in Business',\n",
       "  'Liza on Demand',\n",
       "  'Home Economics',\n",
       "  'DMZ']}"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the response in the form of html\n",
    "wikiurl=\"https://en.wikipedia.org/wiki/Nora_Dunn#Filmography\"\n",
    "table_class=\"wikitable\"\n",
    "response=requests.get(wikiurl)\n",
    "print(response.status_code)\n",
    "\n",
    "# parse data from the html into a beautifulsoup object\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "indiatable=soup.find_all('table',{'class':\"wikitable\"})[1]\n",
    "\n",
    "df=pd.read_html(str(indiatable))\n",
    "# convert list to dataframe\n",
    "df=pd.DataFrame(df[0])\n",
    "Nora_Dunn_film = []\n",
    "for title in df['Title']:\n",
    "    Nora_Dunn_film.append(title)\n",
    "Nora_Dunn_film\n",
    "\n",
    "# parse data from the html into a beautifulsoup object\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "indiatable=soup.find_all('table',{'class':\"wikitable\"})[0]\n",
    "\n",
    "df=pd.read_html(str(indiatable))\n",
    "# convert list to dataframe\n",
    "df=pd.DataFrame(df[0])\n",
    "Nora_Dunn_tv = []\n",
    "for title in df['Title']:\n",
    "    Nora_Dunn_tv.append(title)\n",
    "Nora_Dunn_tv\n",
    "\n",
    "Nora_Dunn_dict = {'Film':Nora_Dunn_film,'TV':Nora_Dunn_tv}\n",
    "Nora_Dunn_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "b28902f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Film': ['Tootsie',\n",
       "  'Amadeus',\n",
       "  'Thief of Hearts',\n",
       "  'Mac and Me',\n",
       "  'Ghost Dad',\n",
       "  'Dead Again',\n",
       "  'Folks!',\n",
       "  'The Lounge People',\n",
       "  'My Girl 2',\n",
       "  'Richie Rich',\n",
       "  'Black Sheep',\n",
       "  'Pie in the Sky',\n",
       "  \"'Til There Was You\",\n",
       "  'My Favorite Martian',\n",
       "  'True Crime',\n",
       "  'Confessions of a Shopaholic',\n",
       "  'The Drawn Together Movie: The Movie',\n",
       "  'The Big Wedding',\n",
       "  'The Wolf of Wall Street',\n",
       "  'Steven Universe: The Movie',\n",
       "  'Driveways',\n",
       "  'Licorice Pizza'],\n",
       " 'TV': [\"Ryan's Hope\",\n",
       "  'Saturday Night Live',\n",
       "  'Love, Sidney',\n",
       "  'One Life to Live',\n",
       "  'The Dollmaker',\n",
       "  'Valerie',\n",
       "  'Acceptable Risks',\n",
       "  'The Cavanaughs',\n",
       "  'American Dreamer',\n",
       "  'Murphy Brown',\n",
       "  'Empty Nest',\n",
       "  'Rachel Gunn, R.N.',\n",
       "  'Dying to Love You',\n",
       "  'Gypsy',\n",
       "  'Hey Arnold!',\n",
       "  'Ally McBeal',\n",
       "  'An Unexpected Family',\n",
       "  'Just Shoot Me!',\n",
       "  'Double Platinum',\n",
       "  'Mary and Rhoda',\n",
       "  'Will & Grace',\n",
       "  'The Electric Piper',\n",
       "  'An Unexpected Love',\n",
       "  'Crossing Jordan',\n",
       "  'Related',\n",
       "  'Cashmere Mafia',\n",
       "  'Boston Legal',\n",
       "  'Lipstick Jungle',\n",
       "  'Law & Order: Special Victims Unit',\n",
       "  'Samantha Who?',\n",
       "  'Royal Pains',\n",
       "  'Ugly Betty',\n",
       "  'Retired at 35',\n",
       "  'Sullivan & Son',\n",
       "  'American Horror Story: Coven',\n",
       "  'Unbreakable Kimmy Schmidt',\n",
       "  'Madam Secretary',\n",
       "  'Crisis in Six Scenes',\n",
       "  'Search Party',\n",
       "  'Pose',\n",
       "  'Steven Universe',\n",
       "  'Blue Bloods',\n",
       "  'Bob Hearts Abishola',\n",
       "  'Steven Universe Future',\n",
       "  'The Kominsky Method']}"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the response in the form of html\n",
    "wikiurl=\"https://en.wikipedia.org/wiki/Christine_Ebersole#Filmography\"\n",
    "table_class=\"wikitable\"\n",
    "response=requests.get(wikiurl)\n",
    "print(response.status_code)\n",
    "\n",
    "# parse data from the html into a beautifulsoup object\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "indiatable=soup.find_all('table',{'class':\"wikitable\"})[0]\n",
    "\n",
    "df=pd.read_html(str(indiatable))\n",
    "# convert list to dataframe\n",
    "df=pd.DataFrame(df[0])\n",
    "Christine_Ebersole_film = []\n",
    "for title in df['Title']:\n",
    "    Christine_Ebersole_film.append(title)\n",
    "Christine_Ebersole_film\n",
    "\n",
    "# parse data from the html into a beautifulsoup object\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "indiatable=soup.find_all('table',{'class':\"wikitable\"})[1]\n",
    "\n",
    "df=pd.read_html(str(indiatable))\n",
    "# convert list to dataframe\n",
    "df=pd.DataFrame(df[0])\n",
    "Christine_Ebersole_tv = []\n",
    "for title in df['Title']:\n",
    "    Christine_Ebersole_tv.append(title)\n",
    "Christine_Ebersole_tv\n",
    "\n",
    "Christine_Ebersole_dict = {'Film':Christine_Ebersole_film,'TV':Christine_Ebersole_tv}\n",
    "Christine_Ebersole_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "22e1267d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Title'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/.conda/envs/DataViz2/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3360\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3361\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3362\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/DataViz2/lib/python3.7/site-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/DataViz2/lib/python3.7/site-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Title'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/5y/htsjngjd7nd_3ddqv2q24ynh0000gq/T/ipykernel_2208/1105246859.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mDean_Edwards_film\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mtitle\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Title'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mDean_Edwards_film\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mDean_Edwards_film\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/DataViz2/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3455\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_single_key\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3456\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3457\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3458\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3459\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/DataViz2/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_getitem_multilevel\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3506\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3507\u001b[0m         \u001b[0;31m# self.columns is a MultiIndex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3508\u001b[0;31m         \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3509\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mslice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3510\u001b[0m             \u001b[0mnew_columns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/DataViz2/lib/python3.7/site-packages/pandas/core/indexes/multi.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method)\u001b[0m\n\u001b[1;32m   2920\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2921\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2922\u001b[0;31m             \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_level_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2923\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_maybe_to_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2924\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/DataViz2/lib/python3.7/site-packages/pandas/core/indexes/multi.py\u001b[0m in \u001b[0;36m_get_level_indexer\u001b[0;34m(self, key, level, indexer)\u001b[0m\n\u001b[1;32m   3202\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3204\u001b[0;31m             \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_loc_single_level_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlevel_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3206\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlevel\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lexsort_depth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/DataViz2/lib/python3.7/site-packages/pandas/core/indexes/multi.py\u001b[0m in \u001b[0;36m_get_loc_single_level_index\u001b[0;34m(self, level_index, key)\u001b[0m\n\u001b[1;32m   2853\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2854\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2855\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mlevel_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2856\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2857\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/DataViz2/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3361\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3362\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3363\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3365\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhasnans\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Title'"
     ]
    }
   ],
   "source": [
    "# get the response in the form of html\n",
    "wikiurl=\"https://en.wikipedia.org/wiki/Dean_Edwards#Filmography\"\n",
    "table_class=\"wikitable\"\n",
    "response=requests.get(wikiurl)\n",
    "print(response.status_code)\n",
    "\n",
    "# parse data from the html into a beautifulsoup object\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "indiatable=soup.find_all('table',{'class':\"wikitable\"})[0]\n",
    "\n",
    "df=pd.read_html(str(indiatable))\n",
    "# convert list to dataframe\n",
    "df=pd.DataFrame(df[0])\n",
    "Dean_Edwards_film = []\n",
    "for title in df['Title']:\n",
    "    Dean_Edwards_film.append(title)\n",
    "Dean_Edwards_film\n",
    "\n",
    "# parse data from the html into a beautifulsoup object\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "indiatable=soup.find_all('table',{'class':\"wikitable\"})[1]\n",
    "\n",
    "df=pd.read_html(str(indiatable))\n",
    "# convert list to dataframe\n",
    "df=pd.DataFrame(df[0])\n",
    "Dean_Edwards_tv = []\n",
    "for title in df['Title']:\n",
    "    Dean_Edwards_tv.append(title)\n",
    "Dean_Edwards_tv\n",
    "\n",
    "Dean_Edwards_dict = {'Film':Dean_Edwards_film,'TV':Dean_Edwards_tv}\n",
    "Dean_Edwards_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa3a9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the response in the form of html\n",
    "wikiurl=\"https://en.wikipedia.org/wiki/\"\n",
    "table_class=\"wikitable\"\n",
    "response=requests.get(wikiurl)\n",
    "print(response.status_code)\n",
    "\n",
    "# parse data from the html into a beautifulsoup object\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "indiatable=soup.find_all('table',{'class':\"wikitable\"})[0]\n",
    "\n",
    "df=pd.read_html(str(indiatable))\n",
    "# convert list to dataframe\n",
    "df=pd.DataFrame(df[0])\n",
    "_film = []\n",
    "for title in df['Title']:\n",
    "    _film.append(title)\n",
    "_film\n",
    "\n",
    "# parse data from the html into a beautifulsoup object\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "indiatable=soup.find_all('table',{'class':\"wikitable\"})[1]\n",
    "\n",
    "df=pd.read_html(str(indiatable))\n",
    "# convert list to dataframe\n",
    "df=pd.DataFrame(df[0])\n",
    "_tv = []\n",
    "for title in df['Title']:\n",
    "    _tv.append(title)\n",
    "_tv\n",
    "\n",
    "_dict = {'Film':_film,'TV':_tv}\n",
    "_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c28729c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the response in the form of html\n",
    "wikiurl=\"https://en.wikipedia.org/wiki/\"\n",
    "table_class=\"wikitable\"\n",
    "response=requests.get(wikiurl)\n",
    "print(response.status_code)\n",
    "\n",
    "# parse data from the html into a beautifulsoup object\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "indiatable=soup.find_all('table',{'class':\"wikitable\"})[0]\n",
    "\n",
    "df=pd.read_html(str(indiatable))\n",
    "# convert list to dataframe\n",
    "df=pd.DataFrame(df[0])\n",
    "_film = []\n",
    "for title in df['Title']:\n",
    "    _film.append(title)\n",
    "_film\n",
    "\n",
    "# parse data from the html into a beautifulsoup object\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "indiatable=soup.find_all('table',{'class':\"wikitable\"})[1]\n",
    "\n",
    "df=pd.read_html(str(indiatable))\n",
    "# convert list to dataframe\n",
    "df=pd.DataFrame(df[0])\n",
    "_tv = []\n",
    "for title in df['Title']:\n",
    "    _tv.append(title)\n",
    "_tv\n",
    "\n",
    "_dict = {'Film':_film,'TV':_tv}\n",
    "_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea6125c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the response in the form of html\n",
    "wikiurl=\"https://en.wikipedia.org/wiki/\"\n",
    "table_class=\"wikitable\"\n",
    "response=requests.get(wikiurl)\n",
    "print(response.status_code)\n",
    "\n",
    "# parse data from the html into a beautifulsoup object\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "indiatable=soup.find_all('table',{'class':\"wikitable\"})[0]\n",
    "\n",
    "df=pd.read_html(str(indiatable))\n",
    "# convert list to dataframe\n",
    "df=pd.DataFrame(df[0])\n",
    "_film = []\n",
    "for title in df['Title']:\n",
    "    _film.append(title)\n",
    "_film\n",
    "\n",
    "# parse data from the html into a beautifulsoup object\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "indiatable=soup.find_all('table',{'class':\"wikitable\"})[1]\n",
    "\n",
    "df=pd.read_html(str(indiatable))\n",
    "# convert list to dataframe\n",
    "df=pd.DataFrame(df[0])\n",
    "_tv = []\n",
    "for title in df['Title']:\n",
    "    _tv.append(title)\n",
    "_tv\n",
    "\n",
    "_dict = {'Film':_film,'TV':_tv}\n",
    "_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f511b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the response in the form of html\n",
    "wikiurl=\"https://en.wikipedia.org/wiki/\"\n",
    "table_class=\"wikitable\"\n",
    "response=requests.get(wikiurl)\n",
    "print(response.status_code)\n",
    "\n",
    "# parse data from the html into a beautifulsoup object\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "indiatable=soup.find_all('table',{'class':\"wikitable\"})[0]\n",
    "\n",
    "df=pd.read_html(str(indiatable))\n",
    "# convert list to dataframe\n",
    "df=pd.DataFrame(df[0])\n",
    "_film = []\n",
    "for title in df['Title']:\n",
    "    _film.append(title)\n",
    "_film\n",
    "\n",
    "# parse data from the html into a beautifulsoup object\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "indiatable=soup.find_all('table',{'class':\"wikitable\"})[1]\n",
    "\n",
    "df=pd.read_html(str(indiatable))\n",
    "# convert list to dataframe\n",
    "df=pd.DataFrame(df[0])\n",
    "_tv = []\n",
    "for title in df['Title']:\n",
    "    _tv.append(title)\n",
    "_tv\n",
    "\n",
    "_dict = {'Film':_film,'TV':_tv}\n",
    "_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0514c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c812ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d60580",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cad991",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e338bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc6e442",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ee9c12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa1fd9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2604967",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c321102",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79ba7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No wiki tables\n",
    "Morwenna_Banks_dict = {'Film':[],'TV':['Saturday Night Live']}\n",
    "A_Whitney_Brown_dict = {'Film':[],'TV':['Saturday Night Live']}\n",
    "Beth_Cahill_dict = {'Film':[],'TV':['Saturday Night Live']}\n",
    "Ellen_Cleghor_dict = {'Film':['Armageddon','Coyote Ugly','Little Nicky','Old School','Grown Ups 2','Second Act'],'TV':['Def Comedy Jam','In Living Color','Cleghorne!','The Adventures of Pete and Pete','Worst Cooks in America: Celebrity Edition','Saturday Night Live']}\n",
    "Denny_Dillion_dict = {'Film':[],'TV':['Saturday Night Live']}\n",
    "Andrew_Dismukes_dict = {'Film':['Call Me Brother','Inside Joke at Moontower'],'TV':['Saturday Night Live']}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7c531a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the response in the form of html\n",
    "wikiurl=\"https://en.wikipedia.org/wiki/\"\n",
    "table_class=\"wikitable\"\n",
    "response=requests.get(wikiurl)\n",
    "print(response.status_code)\n",
    "\n",
    "# parse data from the html into a beautifulsoup object\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "indiatable=soup.find_all('table',{'class':\"wikitable\"})[0]\n",
    "\n",
    "df=pd.read_html(str(indiatable))\n",
    "# convert list to dataframe\n",
    "df=pd.DataFrame(df[0])\n",
    "_film = []\n",
    "for title in df['Title']:\n",
    "    _film.append(title)\n",
    "_film\n",
    "\n",
    "# parse data from the html into a beautifulsoup object\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "indiatable=soup.find_all('table',{'class':\"wikitable\"})[1]\n",
    "\n",
    "df=pd.read_html(str(indiatable))\n",
    "# convert list to dataframe\n",
    "df=pd.DataFrame(df[0])\n",
    "_tv = []\n",
    "for title in df['Title']:\n",
    "    _tv.append(title)\n",
    "_tv\n",
    "\n",
    "_dict = {'Film':_film,'TV':_tv}\n",
    "_dict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
